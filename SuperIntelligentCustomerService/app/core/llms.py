# -*- coding: utf-8 -*-
"""
LLMæ¨¡å‹å®¢æˆ·ç«¯ç®¡ç†å™¨
åŸºäºæ•°æ®åº“é…ç½®åŠ¨æ€åˆ›å»ºå’Œç®¡ç†æ¨¡å‹å®¢æˆ·ç«¯
"""
from typing import Dict, Optional, List

from autogen_core.models import ModelInfo, ModelFamily, ChatCompletionClient
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.models.openai._model_info import _MODEL_INFO, _MODEL_TOKEN_LIMITS


class LLMModelClientManager:
    """LLMæ¨¡å‹å®¢æˆ·ç«¯ç®¡ç†å™¨"""

    def __init__(self):
        self._clients: Dict[str, ChatCompletionClient] = {}
        self._model_info_cache: Dict[str, ModelInfo] = {}
        self._initialized = False

    async def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹å®¢æˆ·ç«¯ç®¡ç†å™¨"""
        if self._initialized:
            return

        # åªä»æ•°æ®åº“åŠ è½½æ¨¡å‹ï¼Œä¸ä½¿ç”¨ç¡¬ç¼–ç é…ç½®
        success = await self.load_models_from_database()
        if not success:
            print("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹é…ç½®ï¼Œè¯·å…ˆåˆå§‹åŒ–æ¨¡å‹æ•°æ®")
            print("ğŸ’¡ æç¤ºï¼šå¯ä»¥é€šè¿‡ç®¡ç†ç•Œé¢æ·»åŠ æ¨¡å‹é…ç½®ï¼Œæˆ–è¿è¡Œæ•°æ®åˆå§‹åŒ–è„šæœ¬")

        self._initialized = True

    async def load_models_from_database(self):
        """ä»æ•°æ®åº“é‡æ–°åŠ è½½æ¨¡å‹é…ç½®ï¼ˆç”¨äºçƒ­é‡è½½ï¼‰"""
        try:
            from tortoise import Tortoise
            if not Tortoise._inited:
                print("æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œæ— æ³•é‡æ–°åŠ è½½")
                return False

            from ..models.llm_models import LLMModel
            models = await LLMModel.filter(is_active=True).order_by("sort_order", "display_name")

            if not models:
                print("æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹é…ç½®")
                return False

            # æ¸…ç©ºç°æœ‰å®¢æˆ·ç«¯
            self._clients.clear()
            self._model_info_cache.clear()

            for model in models:
                try:
                    # åˆ›å»ºæ¨¡å‹ä¿¡æ¯
                    model_info = ModelInfo(
                        vision=model.vision,
                        function_calling=model.function_calling,
                        json_output=model.json_output,
                        structured_output=model.structured_output,
                        multiple_system_messages=model.multiple_system_messages,
                        family=ModelFamily.UNKNOWN
                    )

                    # è§£å¯†APIå¯†é’¥
                    from ..utils.security import decrypt_api_key
                    api_key = decrypt_api_key(model.api_key) if model.api_key else ""

                    # åˆ›å»ºå®¢æˆ·ç«¯
                    client = OpenAIChatCompletionClient(
                        model=model.model_name,
                        base_url=model.base_url,
                        api_key=api_key,
                        model_info=model_info,
                        temperature=model.temperature,
                        top_p=model.top_p,
                        max_tokens=model.max_tokens
                    )

                    # æ³¨å†Œå®¢æˆ·ç«¯
                    client_key = f"{model.provider_name}:{model.model_name}"
                    self._clients[client_key] = client
                    self._clients[model.model_name] = client

                    # ç¼“å­˜æ¨¡å‹ä¿¡æ¯
                    self._model_info_cache[model.model_name] = model_info

                    # æ›´æ–°å…¨å±€æ¨¡å‹ä¿¡æ¯
                    _MODEL_INFO[model.model_name] = model_info
                    _MODEL_TOKEN_LIMITS[model.model_name] = model.max_tokens

                    print(f"âœ… åŠ è½½æ¨¡å‹: {model.display_name}")

                except Exception as e:
                    print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ {model.model_name}: {e}")
                    continue

            print(f"âœ… ä»æ•°æ®åº“åŠ è½½äº† {len(self._clients)} ä¸ªæ¨¡å‹å®¢æˆ·ç«¯")
            return True

        except Exception as e:
            print(f"âŒ ä»æ•°æ®åº“åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False

    async def get_client(self, model_name: str) -> Optional[ChatCompletionClient]:
        """è·å–æ¨¡å‹å®¢æˆ·ç«¯"""
        if not self._initialized:
            await self.initialize()

        return self._clients.get(model_name)

    async def get_default_client(self) -> Optional[ChatCompletionClient]:
        """è·å–é»˜è®¤æ¨¡å‹å®¢æˆ·ç«¯"""
        if not self._initialized:
            await self.initialize()

        # ä»æ•°æ®åº“æŸ¥è¯¢é»˜è®¤æ¨¡å‹
        from tortoise import Tortoise
        if Tortoise._inited:
            from ..models.llm_models import LLMModel
            default_model = await LLMModel.filter(is_active=True, is_default=True).first()
            if default_model:
                return self._clients.get(default_model.model_name)

        # å¦‚æœæ²¡æœ‰è®¾ç½®é»˜è®¤æ¨¡å‹ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„å®¢æˆ·ç«¯
        return next(iter(self._clients.values()), None)

    async def list_available_models(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
        if not self._initialized:
            await self.initialize()

        # ç›´æ¥ä»æ•°æ®åº“æŸ¥è¯¢æ´»è·ƒçš„æ¨¡å‹
        from tortoise import Tortoise
        if Tortoise._inited:
            from ..models.llm_models import LLMModel
            models = await LLMModel.filter(is_active=True).order_by("sort_order", "display_name")
            return [model.model_name for model in models]

        return []

    async def reload_models(self):
        """é‡æ–°åŠ è½½æ¨¡å‹é…ç½®ï¼ˆä»æ•°æ®åº“ï¼‰"""
        success = await self.load_models_from_database()
        if not success:
            print("âŒ é‡æ–°åŠ è½½æ¨¡å‹å¤±è´¥ï¼šæ•°æ®åº“ä¸­æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹é…ç½®")
        return success


# åˆ›å»ºå…¨å±€æ¨¡å‹å®¢æˆ·ç«¯ç®¡ç†å™¨å®ä¾‹
model_client_manager = LLMModelClientManager()


# æ ¸å¿ƒAPIå‡½æ•°
async def get_model_client(model_name: str) -> Optional[ChatCompletionClient]:
    """è·å–æŒ‡å®šçš„æ¨¡å‹å®¢æˆ·ç«¯"""
    return await model_client_manager.get_client(model_name)


async def get_default_model_client() -> Optional[ChatCompletionClient]:
    """è·å–é»˜è®¤æ¨¡å‹å®¢æˆ·ç«¯"""
    return await model_client_manager.get_default_client()


async def list_available_models() -> List[str]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
    return await model_client_manager.list_available_models()


async def initialize_llm_clients():
    """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
    await model_client_manager.initialize()
    print("âœ… LLMå®¢æˆ·ç«¯ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
