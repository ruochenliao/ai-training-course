"""
å‘é‡æ•°æ®åº“é…ç½®
"""
import os
from pathlib import Path
from typing import Dict, Any


class VectorDBConfig:
    """å‘é‡æ•°æ®åº“é…ç½®"""
    
    # ChromaDBé…ç½®
    CHROMA_PERSIST_DIRECTORY: str = os.getenv(
        "CHROMA_PERSIST_DIRECTORY",
        str(Path.home() / ".chromadb_intelligent_customer_service")
    )

    # æ¨¡å‹ç¼“å­˜ç›®å½•é…ç½®
    MODEL_CACHE_DIR: str = os.getenv(
        "MODEL_CACHE_DIR",
        str(Path(__file__).parent.parent.parent / "models")
    )

    # åµŒå…¥æ¨¡å‹é…ç½® - ä½¿ç”¨é­”å¡”ç¤¾åŒºQwen3-0.6Bæ¨¡å‹
    EMBEDDING_MODEL_NAME: str = os.getenv("EMBEDDING_MODEL_NAME", "Qwen3-0.6B")
    EMBEDDING_MODEL_PATH: str = os.getenv(
        "EMBEDDING_MODEL_PATH",
        str(Path(MODEL_CACHE_DIR) / "embedding" / "Qwen3-0.6B")
    )
    EMBEDDING_DIMENSION: int = int(os.getenv("EMBEDDING_DIMENSION", "896"))  # Qwen3-0.6Bçš„åµŒå…¥ç»´åº¦
    USE_LOCAL_EMBEDDING: bool = os.getenv("USE_LOCAL_EMBEDDING", "true").lower() == "true"  # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹

    # é‡æ’æ¨¡å‹é…ç½® - ä½¿ç”¨é­”å¡”ç¤¾åŒºQwen3-Reranker-0.6Bæ¨¡å‹
    RERANKER_MODEL_NAME: str = os.getenv("RERANKER_MODEL_NAME", "Qwen3-Reranker-0.6B")
    RERANKER_MODEL_PATH: str = os.getenv(
        "RERANKER_MODEL_PATH",
        str(Path(MODEL_CACHE_DIR) / "reranker" / "Qwen3-Reranker-0.6B")
    )
    USE_RERANKER: bool = os.getenv("USE_RERANKER", "true").lower() == "true"
    USE_LOCAL_RERANKER: bool = os.getenv("USE_LOCAL_RERANKER", "true").lower() == "true"  # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹
    
    # æ£€ç´¢é…ç½®
    DEFAULT_RETRIEVAL_LIMIT: int = int(os.getenv("DEFAULT_RETRIEVAL_LIMIT", "5"))
    RETRIEVAL_MULTIPLIER: int = int(os.getenv("RETRIEVAL_MULTIPLIER", "3"))  # æ£€ç´¢å€™é€‰æ•°é‡å€æ•°
    SIMILARITY_THRESHOLD: float = float(os.getenv("SIMILARITY_THRESHOLD", "0.1"))
    
    # æ€§èƒ½é…ç½®
    BATCH_SIZE: int = int(os.getenv("VECTOR_BATCH_SIZE", "32"))
    MAX_WORKERS: int = int(os.getenv("MAX_WORKERS", "4"))
    
    # ç¼“å­˜é…ç½®
    ENABLE_MODEL_CACHE: bool = os.getenv("ENABLE_MODEL_CACHE", "true").lower() == "true"
    MODEL_CACHE_SIZE: int = int(os.getenv("MODEL_CACHE_SIZE", "100"))
    
    # é›†åˆé…ç½®
    PRIVATE_MEMORY_COLLECTION_PREFIX: str = "private_memories"
    PUBLIC_MEMORY_COLLECTION_NAME: str = "public_knowledge_base"
    CHAT_MEMORY_COLLECTION_PREFIX: str = "chat_memories"
    
    # é«˜çº§é…ç½®
    ENABLE_HYBRID_SEARCH: bool = os.getenv("ENABLE_HYBRID_SEARCH", "true").lower() == "true"
    HYBRID_SEARCH_ALPHA: float = float(os.getenv("HYBRID_SEARCH_ALPHA", "0.7"))  # å‘é‡æœç´¢æƒé‡
    
    @classmethod
    def get_config(cls) -> Dict[str, Any]:
        """è·å–é…ç½®å­—å…¸"""
        return {
            "chroma_persist_directory": cls.CHROMA_PERSIST_DIRECTORY,
            "model_cache_dir": cls.MODEL_CACHE_DIR,
            "embedding_model_name": cls.EMBEDDING_MODEL_NAME,
            "embedding_model_path": cls.EMBEDDING_MODEL_PATH,
            "embedding_dimension": cls.EMBEDDING_DIMENSION,
            "use_local_embedding": cls.USE_LOCAL_EMBEDDING,
            "reranker_model_name": cls.RERANKER_MODEL_NAME,
            "reranker_model_path": cls.RERANKER_MODEL_PATH,
            "use_reranker": cls.USE_RERANKER,
            "use_local_reranker": cls.USE_LOCAL_RERANKER,
            "default_retrieval_limit": cls.DEFAULT_RETRIEVAL_LIMIT,
            "retrieval_multiplier": cls.RETRIEVAL_MULTIPLIER,
            "similarity_threshold": cls.SIMILARITY_THRESHOLD,
            "batch_size": cls.BATCH_SIZE,
            "max_workers": cls.MAX_WORKERS,
            "enable_model_cache": cls.ENABLE_MODEL_CACHE,
            "model_cache_size": cls.MODEL_CACHE_SIZE,
            "private_memory_collection_prefix": cls.PRIVATE_MEMORY_COLLECTION_PREFIX,
            "public_memory_collection_name": cls.PUBLIC_MEMORY_COLLECTION_NAME,
            "chat_memory_collection_prefix": cls.CHAT_MEMORY_COLLECTION_PREFIX,
            "enable_hybrid_search": cls.ENABLE_HYBRID_SEARCH,
            "hybrid_search_alpha": cls.HYBRID_SEARCH_ALPHA
        }
    
    @classmethod
    def get_chroma_settings(cls) -> Dict[str, Any]:
        """è·å–ChromaDBè®¾ç½®"""
        return {
            "anonymized_telemetry": False,
            "allow_reset": True,
            "is_persistent": True
        }
    
    @classmethod
    def get_embedding_model_config(cls) -> Dict[str, Any]:
        """è·å–åµŒå…¥æ¨¡å‹é…ç½®"""
        return {
            "model_name": cls.EMBEDDING_MODEL_NAME,
            "model_path": cls.EMBEDDING_MODEL_PATH,
            "use_local": cls.USE_LOCAL_EMBEDDING,
            "device": "cpu",  # å¯ä»¥è®¾ç½®ä¸º "cuda" å¦‚æœæœ‰GPU
            "normalize_embeddings": True,
            "encode_kwargs": {
                "batch_size": cls.BATCH_SIZE,
                "show_progress_bar": False
            }
        }
    
    @classmethod
    def get_reranker_config(cls) -> Dict[str, Any]:
        """è·å–é‡æ’æ¨¡å‹é…ç½®"""
        return {
            "model_name": cls.RERANKER_MODEL_NAME,
            "model_path": cls.RERANKER_MODEL_PATH,
            "use_local": cls.USE_LOCAL_RERANKER,
            "device": "cpu",  # å¯ä»¥è®¾ç½®ä¸º "cuda" å¦‚æœæœ‰GPU
            "max_length": 512,
            "batch_size": cls.BATCH_SIZE
        }


# å…¨å±€é…ç½®å®ä¾‹
vector_db_config = VectorDBConfig()


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ - å•ä¾‹æ¨¡å¼ç®¡ç†åµŒå…¥å’Œé‡æ’æ¨¡å‹"""
    
    _instance = None
    _embedding_model = None
    _reranker_model = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def get_embedding_model(self):
        """è·å–åµŒå…¥æ¨¡å‹ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._embedding_model is None:
            from ..utils.qwen_model_loader import create_qwen_embedding_model
            config = vector_db_config.get_embedding_model_config()

            # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°Qwenæ¨¡å‹
            if config["use_local"] and Path(config["model_path"]).exists():
                model_path = config["model_path"]
                print(f"ğŸ”„ åŠ è½½æœ¬åœ°QwenåµŒå…¥æ¨¡å‹: {model_path}")
                self._embedding_model = create_qwen_embedding_model(model_path, config["device"])

                if self._embedding_model:
                    print(f"âœ… æœ¬åœ°QwenåµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
                else:
                    raise Exception("æœ¬åœ°Qwenæ¨¡å‹åŠ è½½å¤±è´¥")
            else:
                # ä»é­”å¡”ç¤¾åŒºåŠ è½½Qwenæ¨¡å‹
                model_name = config["model_name"]
                print(f"ğŸ”„ ä»é­”å¡”ç¤¾åŒºåŠ è½½QwenåµŒå…¥æ¨¡å‹: {model_name}")
                self._embedding_model = create_qwen_embedding_model(f"Qwen/{model_name}", config["device"])

                if self._embedding_model:
                    print(f"âœ… é­”å¡”ç¤¾åŒºQwenåµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
                else:
                    raise Exception("é­”å¡”ç¤¾åŒºQwenæ¨¡å‹åŠ è½½å¤±è´¥")

        return self._embedding_model
    
    def get_reranker_model(self):
        """è·å–é‡æ’æ¨¡å‹ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if not vector_db_config.USE_RERANKER:
            return None

        if self._reranker_model is None:
            try:
                from ..utils.qwen_model_loader import create_qwen_reranker_model
                config = vector_db_config.get_reranker_config()

                # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°Qwené‡æ’æ¨¡å‹
                if config["use_local"] and Path(config["model_path"]).exists():
                    model_path = config["model_path"]
                    print(f"ğŸ”„ åŠ è½½æœ¬åœ°Qwené‡æ’æ¨¡å‹: {model_path}")
                    self._reranker_model = create_qwen_reranker_model(
                        model_path,
                        config["device"],
                        config["max_length"]
                    )

                    if self._reranker_model:
                        print(f"âœ… æœ¬åœ°Qwené‡æ’æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
                    else:
                        raise Exception("æœ¬åœ°Qwené‡æ’æ¨¡å‹åŠ è½½å¤±è´¥")
                else:
                    # å°è¯•ä»é­”å¡”ç¤¾åŒºåŠ è½½Qwené‡æ’æ¨¡å‹
                    model_name = config["model_name"]
                    print(f"ğŸ”„ ä»é­”å¡”ç¤¾åŒºåŠ è½½Qwené‡æ’æ¨¡å‹: {model_name}")
                    self._reranker_model = create_qwen_reranker_model(
                        f"Qwen/{model_name}",
                        config["device"],
                        config["max_length"]
                    )

                    if self._reranker_model:
                        print(f"âœ… é­”å¡”ç¤¾åŒºQwené‡æ’æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
                    else:
                        raise Exception("é­”å¡”ç¤¾åŒºQwené‡æ’æ¨¡å‹åŠ è½½å¤±è´¥")

            except Exception as e:
                print(f"âš ï¸ Qwené‡æ’æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print("âš ï¸ é‡æ’åŠŸèƒ½å°†è¢«ç¦ç”¨ï¼Œä»…ä½¿ç”¨åµŒå…¥æ¨¡å‹è¿›è¡Œæ£€ç´¢")
                self._reranker_model = None
        return self._reranker_model
    
    def clear_cache(self):
        """æ¸…ç†æ¨¡å‹ç¼“å­˜"""
        self._embedding_model = None
        self._reranker_model = None
        print("ğŸ§¹ æ¨¡å‹ç¼“å­˜å·²æ¸…ç†")


# å…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
model_manager = ModelManager()
