#!/usr/bin/env python3
"""
ç³»ç»Ÿä¼˜åŒ–è„šæœ¬
è‡ªåŠ¨æ‰§è¡Œç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å’Œé…ç½®è°ƒä¼˜
"""

import asyncio
import logging
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Any
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.core.cache_manager import get_cache_manager
from app.core.graph_store import get_graph_store
from app.services.monitoring_service import monitoring_service

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SystemOptimizer:
    """ç³»ç»Ÿä¼˜åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨"""
        self.optimization_results = []
        self.performance_baseline = {}
        
    async def run_optimization(self):
        """è¿è¡Œå®Œæ•´çš„ç³»ç»Ÿä¼˜åŒ–"""
        logger.info("ğŸš€ å¼€å§‹ç³»ç»Ÿä¼˜åŒ–...")
        
        try:
            # 1. æ”¶é›†æ€§èƒ½åŸºçº¿
            await self._collect_performance_baseline()
            
            # 2. æ•°æ®åº“ä¼˜åŒ–
            await self._optimize_databases()
            
            # 3. ç¼“å­˜ä¼˜åŒ–
            await self._optimize_cache()
            
            # 4. å†…å­˜ä¼˜åŒ–
            await self._optimize_memory()
            
            # 5. ç½‘ç»œä¼˜åŒ–
            await self._optimize_network()
            
            # 6. æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–
            await self._optimize_filesystem()
            
            # 7. åº”ç”¨é…ç½®ä¼˜åŒ–
            await self._optimize_application_config()
            
            # 8. éªŒè¯ä¼˜åŒ–æ•ˆæœ
            await self._verify_optimization_results()
            
            # 9. ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
            await self._generate_optimization_report()
            
            logger.info("âœ… ç³»ç»Ÿä¼˜åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç³»ç»Ÿä¼˜åŒ–å¤±è´¥: {str(e)}")
            return False
    
    async def _collect_performance_baseline(self):
        """æ”¶é›†æ€§èƒ½åŸºçº¿"""
        logger.info("ğŸ“Š æ”¶é›†æ€§èƒ½åŸºçº¿...")
        
        try:
            # è·å–å½“å‰ç³»ç»ŸæŒ‡æ ‡
            current_metrics = await monitoring_service.get_current_metrics()
            
            self.performance_baseline = {
                "timestamp": time.time(),
                "system": current_metrics.get("system", {}),
                "application": current_metrics.get("application", {}),
                "model": current_metrics.get("model", {})
            }
            
            logger.info("âœ… æ€§èƒ½åŸºçº¿æ”¶é›†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ”¶é›†æ€§èƒ½åŸºçº¿å¤±è´¥: {str(e)}")
    
    async def _optimize_databases(self):
        """ä¼˜åŒ–æ•°æ®åº“"""
        logger.info("ğŸ—„ï¸ ä¼˜åŒ–æ•°æ®åº“...")
        
        optimizations = []
        
        try:
            # Neo4jä¼˜åŒ–
            graph_store = get_graph_store()
            await graph_store.connect()
            
            # æ£€æŸ¥å¹¶åˆ›å»ºç¼ºå¤±çš„ç´¢å¼•
            index_queries = [
                "CREATE INDEX entity_name_idx IF NOT EXISTS FOR (e:Entity) ON (e.name)",
                "CREATE INDEX entity_type_idx IF NOT EXISTS FOR (e:Entity) ON (e.type)",
                "CREATE INDEX entity_kb_idx IF NOT EXISTS FOR (e:Entity) ON (e.knowledge_base_id)",
                "CREATE INDEX document_title_idx IF NOT EXISTS FOR (d:Document) ON (d.title)"
            ]
            
            for query in index_queries:
                try:
                    await graph_store.execute_cypher(query)
                    optimizations.append(f"åˆ›å»ºç´¢å¼•: {query}")
                except Exception as e:
                    logger.warning(f"ç´¢å¼•åˆ›å»ºè·³è¿‡: {str(e)}")
            
            # æ¸…ç†å­¤ç«‹èŠ‚ç‚¹
            cleanup_query = """
            MATCH (n)
            WHERE NOT (n)--()
            AND labels(n) <> ['TempNode']
            DELETE n
            """
            
            try:
                result = await graph_store.execute_cypher(cleanup_query)
                optimizations.append("æ¸…ç†å­¤ç«‹èŠ‚ç‚¹")
            except Exception as e:
                logger.warning(f"æ¸…ç†å­¤ç«‹èŠ‚ç‚¹å¤±è´¥: {str(e)}")
            
            self.optimization_results.append({
                "category": "æ•°æ®åº“ä¼˜åŒ–",
                "optimizations": optimizations,
                "status": "success"
            })
            
            logger.info("âœ… æ•°æ®åº“ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“ä¼˜åŒ–å¤±è´¥: {str(e)}")
            self.optimization_results.append({
                "category": "æ•°æ®åº“ä¼˜åŒ–",
                "optimizations": [],
                "status": "failed",
                "error": str(e)
            })
    
    async def _optimize_cache(self):
        """ä¼˜åŒ–ç¼“å­˜"""
        logger.info("ğŸ’¾ ä¼˜åŒ–ç¼“å­˜...")
        
        optimizations = []
        
        try:
            cache_manager = get_cache_manager()
            await cache_manager.connect()
            
            # è·å–ç¼“å­˜ç»Ÿè®¡
            cache_stats = await cache_manager.get_stats()
            
            # æ¸…ç†è¿‡æœŸç¼“å­˜
            categories_to_clean = ["search", "temp", "session"]
            for category in categories_to_clean:
                cleaned_count = await cache_manager.clear_category(category)
                if cleaned_count > 0:
                    optimizations.append(f"æ¸…ç† {category} ç¼“å­˜: {cleaned_count} ä¸ªé”®")
            
            # ä¼˜åŒ–ç¼“å­˜é…ç½®
            hit_rate = cache_stats.get("cache_stats", {}).get("hit_rate", 0)
            if hit_rate < 50:
                optimizations.append("ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ï¼Œå»ºè®®è°ƒæ•´ç¼“å­˜ç­–ç•¥")
            
            self.optimization_results.append({
                "category": "ç¼“å­˜ä¼˜åŒ–",
                "optimizations": optimizations,
                "status": "success",
                "cache_stats": cache_stats
            })
            
            logger.info("âœ… ç¼“å­˜ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ç¼“å­˜ä¼˜åŒ–å¤±è´¥: {str(e)}")
            self.optimization_results.append({
                "category": "ç¼“å­˜ä¼˜åŒ–",
                "optimizations": [],
                "status": "failed",
                "error": str(e)
            })
    
    async def _optimize_memory(self):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        logger.info("ğŸ§  ä¼˜åŒ–å†…å­˜ä½¿ç”¨...")
        
        optimizations = []
        
        try:
            import gc
            import psutil
            
            # è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            collected = gc.collect()
            if collected > 0:
                optimizations.append(f"åƒåœ¾å›æ”¶: æ¸…ç† {collected} ä¸ªå¯¹è±¡")
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
            if memory_percent > 80:
                optimizations.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–ä»£ç ")
            elif memory_percent < 30:
                optimizations.append("å†…å­˜ä½¿ç”¨ç‡è¾ƒä½ï¼Œå¯ä»¥è€ƒè™‘å¢åŠ ç¼“å­˜å¤§å°")
            
            # æ¸…ç†ç›‘æ§æœåŠ¡çš„å†å²æ•°æ®
            old_metrics_count = len(monitoring_service.metrics_history)
            if old_metrics_count > 1000:
                monitoring_service.metrics_history = monitoring_service.metrics_history[-500:]
                optimizations.append(f"æ¸…ç†ç›‘æ§å†å²æ•°æ®: {old_metrics_count - 500} æ¡")
            
            self.optimization_results.append({
                "category": "å†…å­˜ä¼˜åŒ–",
                "optimizations": optimizations,
                "status": "success",
                "memory_usage": memory_percent
            })
            
            logger.info("âœ… å†…å­˜ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"å†…å­˜ä¼˜åŒ–å¤±è´¥: {str(e)}")
            self.optimization_results.append({
                "category": "å†…å­˜ä¼˜åŒ–",
                "optimizations": [],
                "status": "failed",
                "error": str(e)
            })
    
    async def _optimize_network(self):
        """ä¼˜åŒ–ç½‘ç»œé…ç½®"""
        logger.info("ğŸŒ ä¼˜åŒ–ç½‘ç»œé…ç½®...")
        
        optimizations = []
        
        try:
            # æ£€æŸ¥ç½‘ç»œè¿æ¥
            import psutil
            
            network_stats = psutil.net_io_counters()
            
            # æ£€æŸ¥ç½‘ç»œä½¿ç”¨æƒ…å†µ
            if network_stats.bytes_sent > 1024**3:  # 1GB
                optimizations.append("ç½‘ç»œå‘é€é‡è¾ƒå¤§ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®ä¼ è¾“ä¼˜åŒ–")
            
            if network_stats.bytes_recv > 1024**3:  # 1GB
                optimizations.append("ç½‘ç»œæ¥æ”¶é‡è¾ƒå¤§ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æ¥æ”¶ä¼˜åŒ–")
            
            # ç½‘ç»œè¿æ¥æ•°æ£€æŸ¥
            connections = psutil.net_connections()
            active_connections = len([c for c in connections if c.status == 'ESTABLISHED'])
            
            if active_connections > 100:
                optimizations.append(f"æ´»è·ƒè¿æ¥æ•°è¾ƒå¤š: {active_connections}")
            
            self.optimization_results.append({
                "category": "ç½‘ç»œä¼˜åŒ–",
                "optimizations": optimizations,
                "status": "success",
                "network_stats": {
                    "bytes_sent": network_stats.bytes_sent,
                    "bytes_recv": network_stats.bytes_recv,
                    "active_connections": active_connections
                }
            })
            
            logger.info("âœ… ç½‘ç»œä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ç½‘ç»œä¼˜åŒ–å¤±è´¥: {str(e)}")
            self.optimization_results.append({
                "category": "ç½‘ç»œä¼˜åŒ–",
                "optimizations": [],
                "status": "failed",
                "error": str(e)
            })
    
    async def _optimize_filesystem(self):
        """ä¼˜åŒ–æ–‡ä»¶ç³»ç»Ÿ"""
        logger.info("ğŸ“ ä¼˜åŒ–æ–‡ä»¶ç³»ç»Ÿ...")
        
        optimizations = []
        
        try:
            import shutil
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_dirs = [
                project_root / "temp",
                project_root / "logs" / "old",
                project_root / "uploads" / "temp"
            ]
            
            for temp_dir in temp_dirs:
                if temp_dir.exists():
                    try:
                        shutil.rmtree(temp_dir)
                        temp_dir.mkdir(exist_ok=True)
                        optimizations.append(f"æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
                    except Exception as e:
                        logger.warning(f"æ¸…ç†ç›®å½•å¤±è´¥ {temp_dir}: {str(e)}")
            
            # æ£€æŸ¥ç£ç›˜ç©ºé—´
            import psutil
            disk_usage = psutil.disk_usage('/')
            disk_percent = (disk_usage.used / disk_usage.total) * 100
            
            if disk_percent > 90:
                optimizations.append("ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œå»ºè®®æ¸…ç†æˆ–æ‰©å®¹")
            elif disk_percent > 80:
                optimizations.append("ç£ç›˜ç©ºé—´ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®å®šæœŸæ¸…ç†")
            
            # æ—¥å¿—æ–‡ä»¶è½®è½¬
            log_dir = project_root / "logs"
            if log_dir.exists():
                log_files = list(log_dir.glob("*.log"))
                large_logs = [f for f in log_files if f.stat().st_size > 100 * 1024 * 1024]  # 100MB
                
                for log_file in large_logs:
                    try:
                        # å‹ç¼©å¤§æ—¥å¿—æ–‡ä»¶
                        import gzip
                        with open(log_file, 'rb') as f_in:
                            with gzip.open(f"{log_file}.gz", 'wb') as f_out:
                                shutil.copyfileobj(f_in, f_out)
                        log_file.unlink()
                        optimizations.append(f"å‹ç¼©æ—¥å¿—æ–‡ä»¶: {log_file.name}")
                    except Exception as e:
                        logger.warning(f"å‹ç¼©æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {str(e)}")
            
            self.optimization_results.append({
                "category": "æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–",
                "optimizations": optimizations,
                "status": "success",
                "disk_usage": disk_percent
            })
            
            logger.info("âœ… æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–å¤±è´¥: {str(e)}")
            self.optimization_results.append({
                "category": "æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–",
                "optimizations": [],
                "status": "failed",
                "error": str(e)
            })
    
    async def _optimize_application_config(self):
        """ä¼˜åŒ–åº”ç”¨é…ç½®"""
        logger.info("âš™ï¸ ä¼˜åŒ–åº”ç”¨é…ç½®...")
        
        optimizations = []
        
        try:
            # æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®
            env_vars_to_check = [
                "REDIS_URL",
                "NEO4J_URI",
                "DEEPSEEK_API_KEY",
                "QWEN_API_KEY"
            ]
            
            missing_vars = []
            for var in env_vars_to_check:
                if not os.getenv(var):
                    missing_vars.append(var)
            
            if missing_vars:
                optimizations.append(f"ç¼ºå°‘ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
            
            # æ£€æŸ¥é…ç½®æ–‡ä»¶
            config_files = [
                project_root / ".env",
                project_root / "docker-compose.yml"
            ]
            
            for config_file in config_files:
                if not config_file.exists():
                    optimizations.append(f"ç¼ºå°‘é…ç½®æ–‡ä»¶: {config_file.name}")
            
            # ä¼˜åŒ–å»ºè®®
            optimizations.extend([
                "å»ºè®®å¯ç”¨ç”Ÿäº§ç¯å¢ƒé…ç½®",
                "å»ºè®®é…ç½®æ—¥å¿—è½®è½¬",
                "å»ºè®®å¯ç”¨ç›‘æ§å‘Šè­¦",
                "å»ºè®®é…ç½®å¤‡ä»½ç­–ç•¥"
            ])
            
            self.optimization_results.append({
                "category": "åº”ç”¨é…ç½®ä¼˜åŒ–",
                "optimizations": optimizations,
                "status": "success"
            })
            
            logger.info("âœ… åº”ç”¨é…ç½®ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åº”ç”¨é…ç½®ä¼˜åŒ–å¤±è´¥: {str(e)}")
            self.optimization_results.append({
                "category": "åº”ç”¨é…ç½®ä¼˜åŒ–",
                "optimizations": [],
                "status": "failed",
                "error": str(e)
            })
    
    async def _verify_optimization_results(self):
        """éªŒè¯ä¼˜åŒ–æ•ˆæœ"""
        logger.info("ğŸ” éªŒè¯ä¼˜åŒ–æ•ˆæœ...")
        
        try:
            # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©ç³»ç»Ÿç¨³å®š
            await asyncio.sleep(5)
            
            # æ”¶é›†ä¼˜åŒ–åçš„æŒ‡æ ‡
            current_metrics = await monitoring_service.get_current_metrics()
            
            # æ¯”è¾ƒä¼˜åŒ–å‰åçš„æ€§èƒ½
            comparison = {
                "before": self.performance_baseline,
                "after": {
                    "timestamp": time.time(),
                    "system": current_metrics.get("system", {}),
                    "application": current_metrics.get("application", {}),
                    "model": current_metrics.get("model", {})
                }
            }
            
            self.optimization_results.append({
                "category": "ä¼˜åŒ–æ•ˆæœéªŒè¯",
                "comparison": comparison,
                "status": "success"
            })
            
            logger.info("âœ… ä¼˜åŒ–æ•ˆæœéªŒè¯å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–æ•ˆæœéªŒè¯å¤±è´¥: {str(e)}")
    
    async def _generate_optimization_report(self):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š...")
        
        try:
            report = {
                "optimization_time": time.time(),
                "total_optimizations": len(self.optimization_results),
                "successful_optimizations": len([r for r in self.optimization_results if r["status"] == "success"]),
                "failed_optimizations": len([r for r in self.optimization_results if r["status"] == "failed"]),
                "results": self.optimization_results,
                "recommendations": [
                    "å®šæœŸè¿è¡Œç³»ç»Ÿä¼˜åŒ–è„šæœ¬",
                    "ç›‘æ§ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡",
                    "åŠæ—¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç¼“å­˜",
                    "ä¿æŒæ•°æ®åº“ç´¢å¼•ä¼˜åŒ–",
                    "é…ç½®è‡ªåŠ¨åŒ–ç›‘æ§å‘Šè­¦"
                ]
            }
            
            # ä¿å­˜æŠ¥å‘Š
            report_file = project_root / "optimization_report.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
            # æ‰“å°æ‘˜è¦
            print("\n" + "="*60)
            print("ğŸ‰ ç³»ç»Ÿä¼˜åŒ–å®Œæˆæ‘˜è¦")
            print("="*60)
            print(f"æ€»ä¼˜åŒ–é¡¹ç›®: {report['total_optimizations']}")
            print(f"æˆåŠŸé¡¹ç›®: {report['successful_optimizations']}")
            print(f"å¤±è´¥é¡¹ç›®: {report['failed_optimizations']}")
            print("\nä¼˜åŒ–ç±»åˆ«:")
            
            for result in self.optimization_results:
                status_icon = "âœ…" if result["status"] == "success" else "âŒ"
                print(f"  {status_icon} {result['category']}: {len(result.get('optimizations', []))} é¡¹ä¼˜åŒ–")
            
            print(f"\nè¯¦ç»†æŠ¥å‘Š: {report_file}")
            print("="*60)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Šå¤±è´¥: {str(e)}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ™ºèƒ½å®¢æœç³»ç»Ÿä¼˜åŒ–å·¥å…·")
    print("="*50)
    
    optimizer = SystemOptimizer()
    success = await optimizer.run_optimization()
    
    if success:
        print("\nğŸ‰ ç³»ç»Ÿä¼˜åŒ–æˆåŠŸå®Œæˆï¼")
        return 0
    else:
        print("\nâŒ ç³»ç»Ÿä¼˜åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
