"""
# Copyright (c) 2025 å·¦å²š. All rights reserved.

å‘é‡åŒ–ç®¡ç†å™¨

è´Ÿè´£æ–‡æœ¬çš„å‘é‡åŒ–å¤„ç†ï¼Œæ”¯æŒå¤šç§åµŒå…¥æ¨¡å‹ã€‚
"""

# # Standard library imports
import asyncio
from concurrent.futures import ThreadPoolExecutor
import hashlib
import json
import logging
import threading
from typing import Any, Dict, List, Optional, Tuple

# # Third-party imports
import numpy as np
from openai import AsyncOpenAI

# # Local application imports
from app.core.config import settings

logger = logging.getLogger(__name__)


class EmbeddingModel:
    """åµŒå…¥æ¨¡å‹åŸºç±»"""
    
    def __init__(self, model_name: str, dimension: int):
        self.model_name = model_name
        self.dimension = dimension
    
    async def embed_text(self, text: str) -> List[float]:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        raise NotImplementedError
    
    async def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        raise NotImplementedError


class OpenAIEmbedding(EmbeddingModel):
    """OpenAIåµŒå…¥æ¨¡å‹"""
    
    def __init__(self, model_name: str = "text-embedding-3-small", api_key: str = None):
        # æ ¹æ®æ¨¡å‹è®¾ç½®ç»´åº¦
        dimensions = {
            "text-embedding-3-small": 1536,
            "text-embedding-3-large": 3072,
            "text-embedding-ada-002": 1536
        }
        
        super().__init__(model_name, dimensions.get(model_name, 1536))
        
        self.client = AsyncOpenAI(
            api_key=api_key or settings.OPENAI_API_KEY,
            base_url=settings.OPENAI_BASE_URL
        )
    
    async def embed_text(self, text: str) -> List[float]:
        """å°†å•ä¸ªæ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        try:
            response = await self.client.embeddings.create(
                model=self.model_name,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"OpenAIåµŒå…¥å¤±è´¥: {e}")
            raise
    
    async def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        try:
            # OpenAI APIæ”¯æŒæ‰¹é‡å¤„ç†ï¼Œä½†æœ‰é™åˆ¶
            batch_size = 100
            all_embeddings = []
            
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                response = await self.client.embeddings.create(
                    model=self.model_name,
                    input=batch
                )
                
                batch_embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(batch_embeddings)
            
            return all_embeddings
        except Exception as e:
            logger.error(f"OpenAIæ‰¹é‡åµŒå…¥å¤±è´¥: {e}")
            raise


class LocalEmbedding(EmbeddingModel):
    """æœ¬åœ°åµŒå…¥æ¨¡å‹ï¼ˆä½¿ç”¨sentence-transformersï¼‰"""

    def __init__(self, model_name: str = "BAAI/bge-small-zh-v1.5"):  # ä½¿ç”¨å°å‹ä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼ŒåŠ è½½æ›´å¿«
        try:
            # # Standard library imports
            import os

            # # Third-party imports
            from sentence_transformers import SentenceTransformer

            # è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•åˆ°é¡¹ç›®çš„modelsæ–‡ä»¶å¤¹
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
            models_dir = os.path.join(project_root, "models")
            os.makedirs(models_dir, exist_ok=True)

            # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©sentence-transformersä½¿ç”¨æˆ‘ä»¬çš„modelsç›®å½•
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = models_dir

            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
            model_path = os.path.join(models_dir, model_name.replace("/", "_"))
            if os.path.exists(model_path):
                logger.info(f"ğŸ“‚ åŠ è½½æœ¬åœ°æ¨¡å‹: {model_name}")
            else:
                logger.info(f"ğŸ“¥ ä¸‹è½½æ¨¡å‹: {model_name}")

            self.model = SentenceTransformer(model_name, cache_folder=models_dir)
            super().__init__(model_name, self.model.get_sentence_embedding_dimension())
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£…sentence-transformers: pip install sentence-transformers")


class BGEReranker:
    """BGEé‡æ’æ¨¡å‹"""

    def __init__(self, model_name: str = "BAAI/bge-reranker-v2-m3"):
        try:
            # # Standard library imports
            import os

            # # Third-party imports
            from sentence_transformers import CrossEncoder

            # è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
            models_dir = os.path.join(project_root, "models")
            os.makedirs(models_dir, exist_ok=True)
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = models_dir

            self.model_name = model_name
            self.model = CrossEncoder(model_name, cache_folder=models_dir)
            logger.info(f"âœ… BGEé‡æ’æ¨¡å‹åŠ è½½å®Œæˆ: {model_name}")

        except ImportError:
            raise ImportError("éœ€è¦å®‰è£…sentence-transformers: pip install sentence-transformers")
        except Exception as e:
            logger.error(f"BGEé‡æ’æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    async def rerank(self, query: str, documents: List[str], top_k: int = 10) -> List[Tuple[int, float]]:
        """é‡æ’æ–‡æ¡£

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›å‰kä¸ªç»“æœ

        Returns:
            List[Tuple[int, float]]: (åŸå§‹ç´¢å¼•, é‡æ’åˆ†æ•°) çš„åˆ—è¡¨ï¼ŒæŒ‰åˆ†æ•°é™åºæ’åˆ—
        """
        try:
            if not documents:
                return []

            # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹
            pairs = [(query, doc) for doc in documents]

            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œé‡æ’ï¼ˆå› ä¸ºCrossEncoderæ˜¯åŒæ­¥çš„ï¼‰
            loop = asyncio.get_event_loop()
            scores = await loop.run_in_executor(None, self.model.predict, pairs)

            # åˆ›å»º(ç´¢å¼•, åˆ†æ•°)å¯¹å¹¶æ’åº
            indexed_scores = [(i, float(score)) for i, score in enumerate(scores)]
            indexed_scores.sort(key=lambda x: x[1], reverse=True)

            return indexed_scores[:top_k]

        except Exception as e:
            logger.error(f"BGEé‡æ’å¤±è´¥: {e}")
            # è¿”å›åŸå§‹é¡ºåº
            return [(i, 0.0) for i in range(min(len(documents), top_k))]
    
    async def embed_text(self, text: str) -> List[float]:
        """å°†å•ä¸ªæ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        try:
            # sentence-transformersæ˜¯åŒæ­¥çš„ï¼Œåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œ
            loop = asyncio.get_event_loop()
            embedding = await loop.run_in_executor(
                None, self.model.encode, text
            )
            return embedding.tolist()
        except Exception as e:
            logger.error(f"æœ¬åœ°åµŒå…¥å¤±è´¥: {e}")
            raise
    
    async def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        try:
            loop = asyncio.get_event_loop()
            embeddings = await loop.run_in_executor(
                None, self.model.encode, texts
            )
            return embeddings.tolist()
        except Exception as e:
            logger.error(f"æœ¬åœ°æ‰¹é‡åµŒå…¥å¤±è´¥: {e}")
            raise


class EmbeddingCache:
    """åµŒå…¥ç¼“å­˜"""
    
    def __init__(self, max_size: int = 10000):
        self.cache: Dict[str, List[float]] = {}
        self.max_size = max_size
    
    def _get_cache_key(self, text: str, model_name: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{model_name}:{text}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, text: str, model_name: str) -> Optional[List[float]]:
        """è·å–ç¼“å­˜çš„åµŒå…¥"""
        key = self._get_cache_key(text, model_name)
        return self.cache.get(key)
    
    def set(self, text: str, model_name: str, embedding: List[float]):
        """è®¾ç½®ç¼“å­˜çš„åµŒå…¥"""
        if len(self.cache) >= self.max_size:
            # ç®€å•çš„LRUï¼šåˆ é™¤ç¬¬ä¸€ä¸ªå…ƒç´ 
            first_key = next(iter(self.cache))
            del self.cache[first_key]
        
        key = self._get_cache_key(text, model_name)
        self.cache[key] = embedding
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()



class EmbeddingManager:
    """åµŒå…¥ç®¡ç†å™¨"""

    def __init__(self, default_model: str = "bge-zh"):  # ç›´æ¥ä½¿ç”¨BGEä¸­æ–‡æ¨¡å‹ä½œä¸ºé»˜è®¤
        self.models: Dict[str, EmbeddingModel] = {}
        self.reranker: Optional[BGEReranker] = None  # é‡æ’æ¨¡å‹
        self.default_model = default_model
        self.cache = EmbeddingCache()
        self._loading_models = set()  # æ­£åœ¨åŠ è½½çš„æ¨¡å‹
        self._loaded_models = set()  # å·²åŠ è½½çš„æ¨¡å‹
        self._executor = ThreadPoolExecutor(max_workers=2)  # åå°åŠ è½½çº¿ç¨‹æ± 

        # åˆå§‹åŒ–é»˜è®¤æ¨¡å‹
        self._initialize_models()

    def _is_model_downloaded(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½åˆ°æœ¬åœ°"""
        try:
            # # Standard library imports
            import os
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
            models_dir = os.path.join(project_root, "models")

            # Hugging Faceæ¨¡å‹çš„å­˜å‚¨æ ¼å¼æ˜¯ models--org--model-name
            hf_model_path = os.path.join(models_dir, f"models--{model_name.replace('/', '--')}")

            # æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
            if os.path.exists(hf_model_path):
                # æ£€æŸ¥snapshotsç›®å½•æ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºç©º
                snapshots_dir = os.path.join(hf_model_path, "snapshots")
                if os.path.exists(snapshots_dir):
                    snapshots = os.listdir(snapshots_dir)
                    if snapshots:
                        # æ£€æŸ¥æœ€æ–°çš„snapshotæ˜¯å¦åŒ…å«æ¨¡å‹æ–‡ä»¶
                        latest_snapshot = os.path.join(snapshots_dir, snapshots[0])
                        if os.path.exists(latest_snapshot):
                            files = os.listdir(latest_snapshot)
                            has_config = any(f.startswith('config') and f.endswith('.json') for f in files)
                            has_model = any(f.endswith(('.bin', '.safetensors')) for f in files)
                            return has_config and has_model
            return False
        except Exception:
            return False
    
    def _initialize_models(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            # BGEä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼ˆåå°å¼‚æ­¥åŠ è½½ï¼‰
            logger.info("BGEåµŒå…¥æ¨¡å‹å°†åœ¨åå°åŠ è½½...")

            # OpenAIæ¨¡å‹ï¼ˆå¦‚æœé…ç½®äº†APIå¯†é’¥ï¼‰
            if settings.OPENAI_API_KEY:
                try:
                    self.models["openai"] = OpenAIEmbedding()
                    self.models["openai-small"] = OpenAIEmbedding("text-embedding-3-small")
                    self.models["openai-large"] = OpenAIEmbedding("text-embedding-3-large")
                except Exception as e:
                    logger.warning(f"OpenAIåµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")

            logger.info(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {list(self.models.keys())}")
            logger.info(f"é»˜è®¤åµŒå…¥æ¨¡å‹: {self.default_model}")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")

    def start_background_bge_loading(self):
        """å¯åŠ¨åå°BGEæ¨¡å‹ä¸‹è½½å’ŒåŠ è½½ä»»åŠ¡"""
        self._start_background_loading()

    def _start_background_loading(self):
        """å†…éƒ¨æ–¹æ³•ï¼šå¯åŠ¨åå°BGEæ¨¡å‹åŠ è½½ä»»åŠ¡"""
        def load_bge_models():
            """æ™ºèƒ½åŠ è½½BGEæ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹ï¼‰"""
            try:
                # ä¼˜å…ˆåŠ è½½å°å‹BGEä¸­æ–‡æ¨¡å‹ï¼ˆæ›´å¿«ï¼‰
                if "bge-zh" not in self.models and "bge-zh" not in self._loaded_models:
                    self._loading_models.add("bge-zh")
                    try:
                        model_name = "BAAI/bge-small-zh-v1.5"
                        if self._is_model_downloaded(model_name):
                            logger.info("ğŸ“‚ åŠ è½½æœ¬åœ°BGEä¸­æ–‡æ¨¡å‹...")
                        else:
                            logger.info("ğŸ“¥ ä¸‹è½½BGEä¸­æ–‡æ¨¡å‹...")

                        bge_zh_model = LocalEmbedding(model_name)
                        self.models["bge-zh"] = bge_zh_model
                        self._loaded_models.add("bge-zh")
                        logger.info("âœ… BGEä¸­æ–‡æ¨¡å‹å°±ç»ª")

                        # è®¾ç½®BGEæ¨¡å‹ä¸ºé»˜è®¤æ¨¡å‹
                        self.default_model = "bge-zh"
                        logger.info("ğŸ”„ BGEåµŒå…¥æ¨¡å‹å·²å°±ç»ª")
                    except Exception as e:
                        logger.error(f"âŒ BGEä¸­æ–‡æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    finally:
                        self._loading_models.discard("bge-zh")

                # ç„¶ååŠ è½½å¤§å‹BGEä¸­æ–‡æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
                if "bge-large" not in self.models and "bge-large" not in self._loaded_models:
                    self._loading_models.add("bge-large")
                    try:
                        model_name = "BAAI/bge-large-zh-v1.5"
                        if self._is_model_downloaded(model_name):
                            logger.info("ğŸ“‚ åŠ è½½æœ¬åœ°BGEå¤§å‹æ¨¡å‹...")
                        else:
                            logger.info("ğŸ“¥ ä¸‹è½½BGEå¤§å‹æ¨¡å‹...")

                        bge_large_model = LocalEmbedding(model_name)
                        self.models["bge-large"] = bge_large_model
                        self._loaded_models.add("bge-large")
                        logger.info("âœ… BGEå¤§å‹æ¨¡å‹å°±ç»ª")
                    except Exception as e:
                        logger.error(f"âŒ BGEå¤§å‹æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    finally:
                        self._loading_models.discard("bge-large")

                # åŠ è½½BGEé‡æ’æ¨¡å‹
                if self.reranker is None:
                    try:
                        reranker_model_name = "BAAI/bge-reranker-v2-m3"
                        if self._is_model_downloaded(reranker_model_name):
                            logger.info("ğŸ“‚ åŠ è½½æœ¬åœ°BGEé‡æ’æ¨¡å‹...")
                        else:
                            logger.info("ğŸ“¥ ä¸‹è½½BGEé‡æ’æ¨¡å‹...")

                        self.reranker = BGEReranker(reranker_model_name)
                        logger.info("âœ… BGEé‡æ’æ¨¡å‹å°±ç»ª")
                    except Exception as e:
                        logger.error(f"âŒ BGEé‡æ’æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

                logger.info(f"ğŸ‰ æ¨¡å‹åŠ è½½å®Œæˆ - åµŒå…¥: {len(self.models)}ä¸ª, é‡æ’: {'âœ…' if self.reranker else 'âŒ'}")

            except Exception as e:
                logger.error(f"âŒ BGEæ¨¡å‹åŠ è½½å¤±è´¥: {e}")

        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡ŒåŠ è½½ä»»åŠ¡
        self._executor.submit(load_bge_models)

    def get_model_status(self) -> Dict[str, str]:
        """è·å–æ¨¡å‹åŠ è½½çŠ¶æ€"""
        status = {}
        for model_name in ["bge-zh", "bge-large"]:
            if model_name in self._loading_models:
                status[model_name] = "loading"
            elif model_name in self.models:
                status[model_name] = "ready"
            else:
                status[model_name] = "not_loaded"

        # æ·»åŠ é‡æ’æ¨¡å‹çŠ¶æ€
        status["reranker"] = "ready" if self.reranker else "not_loaded"
        return status
    
    def get_model(self, model_name: str = None) -> EmbeddingModel:
        """è·å–åµŒå…¥æ¨¡å‹"""
        model_name = model_name or self.default_model
        if model_name not in self.models:
            raise ValueError(f"æœªæ‰¾åˆ°åµŒå…¥æ¨¡å‹: {model_name}")
        return self.models[model_name]
    
    async def embed_text(self, text: str, model_name: str = None, use_cache: bool = True) -> List[float]:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        model = self.get_model(model_name)
        
        # æ£€æŸ¥ç¼“å­˜
        if use_cache:
            cached_embedding = self.cache.get(text, model.model_name)
            if cached_embedding is not None:
                return cached_embedding
        
        # ç”ŸæˆåµŒå…¥
        embedding = await model.embed_text(text)
        
        # ç¼“å­˜ç»“æœ
        if use_cache:
            self.cache.set(text, model.model_name, embedding)
        
        return embedding
    
    async def embed_texts(self, texts: List[str], model_name: str = None, 
                         use_cache: bool = True) -> List[List[float]]:
        """æ‰¹é‡å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        model = self.get_model(model_name)
        
        if not use_cache:
            return await model.embed_texts(texts)
        
        # æ£€æŸ¥ç¼“å­˜å¹¶åˆ†ç¦»å·²ç¼“å­˜å’Œæœªç¼“å­˜çš„æ–‡æœ¬
        cached_embeddings = {}
        uncached_texts = []
        uncached_indices = []
        
        for i, text in enumerate(texts):
            cached_embedding = self.cache.get(text, model.model_name)
            if cached_embedding is not None:
                cached_embeddings[i] = cached_embedding
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
        
        # ä¸ºæœªç¼“å­˜çš„æ–‡æœ¬ç”ŸæˆåµŒå…¥
        if uncached_texts:
            new_embeddings = await model.embed_texts(uncached_texts)
            
            # ç¼“å­˜æ–°ç”Ÿæˆçš„åµŒå…¥
            for text, embedding in zip(uncached_texts, new_embeddings):
                self.cache.set(text, model.model_name, embedding)
            
            # å°†æ–°åµŒå…¥æ·»åŠ åˆ°ç»“æœä¸­
            for i, embedding in zip(uncached_indices, new_embeddings):
                cached_embeddings[i] = embedding
        
        # æŒ‰åŸå§‹é¡ºåºè¿”å›ç»“æœ
        return [cached_embeddings[i] for i in range(len(texts))]
    
    def get_dimension(self, model_name: str = None) -> int:
        """è·å–æ¨¡å‹çš„å‘é‡ç»´åº¦"""
        model = self.get_model(model_name)
        return model.dimension
    
    def list_models(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹"""
        return list(self.models.keys())
    
    async def similarity(self, text1: str, text2: str, model_name: str = None) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        embeddings = await self.embed_texts([text1, text2], model_name)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        vec1 = np.array(embeddings[0])
        vec2 = np.array(embeddings[1])
        
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return float(dot_product / (norm1 * norm2))
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        logger.info("åµŒå…¥ç¼“å­˜å·²æ¸…ç©º")

    async def rerank_documents(self, query: str, documents: List[str], top_k: int = 10) -> List[Tuple[int, float]]:
        """ä½¿ç”¨BGEé‡æ’æ¨¡å‹å¯¹æ–‡æ¡£è¿›è¡Œé‡æ’

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›å‰kä¸ªç»“æœ

        Returns:
            List[Tuple[int, float]]: (åŸå§‹ç´¢å¼•, é‡æ’åˆ†æ•°) çš„åˆ—è¡¨ï¼ŒæŒ‰åˆ†æ•°é™åºæ’åˆ—
        """
        if self.reranker is None:
            logger.warning("é‡æ’æ¨¡å‹æœªåŠ è½½ï¼Œè¿”å›åŸå§‹é¡ºåº")
            return [(i, 0.0) for i in range(min(len(documents), top_k))]

        return await self.reranker.rerank(query, documents, top_k)

    def has_reranker(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„é‡æ’æ¨¡å‹"""
        return self.reranker is not None


class TokenCounter:
    """Tokenè®¡æ•°å™¨"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def count_tokens(self, text: str, model: str = "gpt-3.5-turbo") -> int:
        """
        è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡

        Args:
            text: è¾“å…¥æ–‡æœ¬
            model: æ¨¡å‹åç§°

        Returns:
            tokenæ•°é‡
        """
        try:
            # ç®€å•çš„tokenä¼°ç®—ï¼Œå®é™…åº”è¯¥ä½¿ç”¨tiktokenåº“
            # å¤§çº¦æ¯4ä¸ªå­—ç¬¦ä¸º1ä¸ªtoken
            return len(text) // 4
        except Exception as e:
            self.logger.error(f"è®¡ç®—tokenå¤±è´¥: {e}")
            return 0

    def count_tokens_batch(self, texts: List[str], model: str = "gpt-3.5-turbo") -> List[int]:
        """
        æ‰¹é‡è®¡ç®—tokenæ•°é‡

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            model: æ¨¡å‹åç§°

        Returns:
            tokenæ•°é‡åˆ—è¡¨
        """
        return [self.count_tokens(text, model) for text in texts]


# å…¨å±€åµŒå…¥ç®¡ç†å™¨
embedding_manager = EmbeddingManager()
token_counter = TokenCounter()
