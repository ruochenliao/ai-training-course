# 智能查询分析服务功能还原提示词

## 服务概述

智能查询分析服务是Text2SQL系统的核心入口模块，负责理解和分析用户的自然语言查询，提取关键信息并为后续的SQL生成提供结构化的分析结果。

## 核心功能

### 1. 自然语言理解
- 解析用户输入的自然语言查询
- 识别查询中的关键实体、概念和业务术语
- 理解查询的语义结构和逻辑关系
- 支持中英文混合查询的处理

### 2. 查询意图识别
- 分析查询类型（统计、筛选、排序、聚合、对比等）
- 识别时间相关的查询条件和时间范围
- 检测数值比较操作和范围查询
- 推断所需的聚合函数（COUNT、SUM、AVG、MAX、MIN等）

### 3. 实体提取与映射
- 从自然语言中提取数据库相关的实体
- 识别可能的表名、字段名和业务概念
- 处理同义词、别名和模糊匹配
- 建立实体与数据库对象的映射关系

### 4. 关系推理
- 分析实体间的关联关系
- 推理可能需要的表连接操作
- 识别层次结构和分组关系
- 理解查询的业务逻辑流程

## 技术实现

### 核心组件架构

```python
class QueryAnalyzerAgent:
    """查询分析智能体"""
    
    def __init__(self, db_type: str, model_client):
        self.db_type = db_type
        self.model_client = model_client
        self.analysis_cache = {}
    
    async def analyze_query(self, query: str) -> Dict[str, Any]:
        """分析自然语言查询"""
        # 检查缓存
        if query in self.analysis_cache:
            return self.analysis_cache[query]
        
        # 使用LLM进行分析
        analysis_result = await self._llm_analyze(query)
        
        # 缓存结果
        self.analysis_cache[query] = analysis_result
        return analysis_result
    
    async def _llm_analyze(self, query: str) -> Dict[str, Any]:
        """使用大语言模型分析查询"""
        prompt = self._build_analysis_prompt(query)
        response = await self.model_client.complete(prompt)
        return self._parse_analysis_response(response)
```

### 分析提示词模板

```python
def _build_analysis_prompt(self, query: str) -> str:
    """构建查询分析提示词"""
    return f"""
    你是一名专业的数据库查询分析专家，请分析以下自然语言查询：
    
    查询: "{query}"
    
    请按照以下JSON格式提供详细分析：
    {{
        "entities": ["查询中提到的实体列表"],
        "relationships": ["实体间的关系描述"],
        "query_intent": "查询的主要目的和意图",
        "query_type": "查询类型（select/aggregate/filter/sort等）",
        "likely_aggregations": ["可能需要的聚合操作"],
        "time_related": "是否涉及时间条件",
        "comparison_related": "是否涉及数值比较",
        "filter_conditions": ["可能的过滤条件"],
        "sort_requirements": ["排序需求"],
        "business_context": "业务上下文和背景"
    }}
    
    分析要求：
    1. 准确识别所有相关实体
    2. 理解查询的业务逻辑
    3. 推断隐含的查询条件
    4. 考虑可能的歧义和多种解释
    """
```

### 关键词提取（回退机制）

```python
def extract_keywords(self, query: str) -> List[str]:
    """提取关键词作为回退机制"""
    import re
    
    # 停用词列表
    stop_words = {
        'the', 'and', 'for', 'from', 'where', 'what', 'which', 'when', 'who',
        'how', 'many', 'much', 'with', 'that', 'this', 'these', 'those',
        '什么', '哪个', '哪些', '什么时候', '谁', '怎么', '多少', '和', '的', '是'
    }
    
    # 提取单词
    keywords = re.findall(r'\b\w+\b', query.lower())
    
    # 过滤停用词和短词
    return [k for k in keywords if len(k) > 2 and k not in stop_words]
```

## 输入输出规范

### 输入格式
```python
{
    "query": "查找销售额最高的前10个产品",
    "context": {
        "user_id": "user123",
        "session_id": "session456",
        "database_type": "MySQL"
    }
}
```

### 输出格式
```python
{
    "entities": ["销售额", "产品"],
    "relationships": ["产品具有销售额属性"],
    "query_intent": "查找销售表现最好的产品排行榜",
    "query_type": "aggregate_sort",
    "likely_aggregations": ["SUM", "MAX"],
    "time_related": false,
    "comparison_related": true,
    "filter_conditions": [],
    "sort_requirements": ["按销售额降序", "限制前10条"],
    "business_context": "销售分析和产品排名",
    "confidence_score": 0.95,
    "alternative_interpretations": [
        "可能指的是销售数量而非销售额",
        "时间范围可能需要明确"
    ]
}
```

## 性能优化

### 缓存策略
```python
class AnalysisCache:
    """分析结果缓存"""
    
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.max_size = max_size
        self.access_count = {}
    
    def get(self, query: str) -> Optional[Dict]:
        """获取缓存结果"""
        if query in self.cache:
            self.access_count[query] = self.access_count.get(query, 0) + 1
            return self.cache[query]
        return None
    
    def put(self, query: str, result: Dict):
        """存储分析结果"""
        if len(self.cache) >= self.max_size:
            self._evict_least_used()
        
        self.cache[query] = result
        self.access_count[query] = 1
```

### 批量处理
```python
async def batch_analyze(self, queries: List[str]) -> List[Dict]:
    """批量分析查询"""
    tasks = []
    for query in queries:
        task = asyncio.create_task(self.analyze_query(query))
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    return results
```

## 错误处理

### 异常处理机制
```python
class QueryAnalysisError(Exception):
    """查询分析异常"""
    pass

class QueryAnalyzer:
    async def analyze_query(self, query: str) -> Dict[str, Any]:
        try:
            # 主要分析逻辑
            return await self._llm_analyze(query)
        except Exception as e:
            logger.error(f"LLM分析失败: {e}")
            # 回退到关键词提取
            return self._fallback_analysis(query)
    
    def _fallback_analysis(self, query: str) -> Dict[str, Any]:
        """回退分析方法"""
        keywords = self.extract_keywords(query)
        return {
            "entities": keywords,
            "relationships": [],
            "query_intent": query,
            "query_type": "unknown",
            "likely_aggregations": [],
            "time_related": self._detect_time_keywords(query),
            "comparison_related": self._detect_comparison_keywords(query),
            "confidence_score": 0.3,
            "fallback_used": True
        }
```

## 质量评估

### 分析质量指标
```python
class AnalysisQualityMetrics:
    """分析质量评估"""
    
    def evaluate_analysis(self, query: str, analysis: Dict, 
                         ground_truth: Dict = None) -> Dict:
        """评估分析质量"""
        metrics = {
            "completeness": self._check_completeness(analysis),
            "consistency": self._check_consistency(analysis),
            "confidence": analysis.get("confidence_score", 0),
            "processing_time": analysis.get("processing_time", 0)
        }
        
        if ground_truth:
            metrics["accuracy"] = self._calculate_accuracy(analysis, ground_truth)
        
        return metrics
    
    def _check_completeness(self, analysis: Dict) -> float:
        """检查分析完整性"""
        required_fields = [
            "entities", "query_intent", "query_type"
        ]
        
        present_fields = sum(1 for field in required_fields 
                           if field in analysis and analysis[field])
        
        return present_fields / len(required_fields)
```

## 集成接口

### REST API接口
```python
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

router = APIRouter()

class QueryAnalysisRequest(BaseModel):
    query: str
    context: Optional[Dict] = None

class QueryAnalysisResponse(BaseModel):
    analysis: Dict[str, Any]
    metrics: Dict[str, float]
    timestamp: datetime

@router.post("/analyze", response_model=QueryAnalysisResponse)
async def analyze_query(request: QueryAnalysisRequest):
    """分析自然语言查询"""
    try:
        analyzer = QueryAnalyzerAgent()
        analysis = await analyzer.analyze_query(request.query)
        
        metrics = AnalysisQualityMetrics().evaluate_analysis(
            request.query, analysis
        )
        
        return QueryAnalysisResponse(
            analysis=analysis,
            metrics=metrics,
            timestamp=datetime.now()
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### 流式处理接口
```python
async def stream_analysis(self, query: str, 
                         callback: Callable[[Dict], None]):
    """流式分析处理"""
    # 发送开始信号
    await callback({
        "type": "analysis_start",
        "query": query,
        "timestamp": datetime.now().isoformat()
    })
    
    # 执行分析
    analysis = await self.analyze_query(query)
    
    # 发送结果
    await callback({
        "type": "analysis_complete",
        "analysis": analysis,
        "timestamp": datetime.now().isoformat()
    })
```

## 测试用例

### 单元测试
```python
import pytest
from unittest.mock import AsyncMock

class TestQueryAnalyzer:
    @pytest.fixture
    def analyzer(self):
        return QueryAnalyzerAgent("MySQL", AsyncMock())
    
    @pytest.mark.asyncio
    async def test_simple_query_analysis(self, analyzer):
        """测试简单查询分析"""
        query = "查找所有用户"
        result = await analyzer.analyze_query(query)
        
        assert "entities" in result
        assert "用户" in result["entities"]
        assert result["query_type"] == "select"
    
    @pytest.mark.asyncio
    async def test_aggregation_query(self, analyzer):
        """测试聚合查询分析"""
        query = "统计每个部门的员工数量"
        result = await analyzer.analyze_query(query)
        
        assert "COUNT" in result["likely_aggregations"]
        assert result["query_type"] == "aggregate"
    
    def test_keyword_extraction(self, analyzer):
        """测试关键词提取"""
        query = "查找销售额最高的产品"
        keywords = analyzer.extract_keywords(query)
        
        assert "销售额" in keywords
        assert "产品" in keywords
        assert "查找" not in keywords  # 停用词应被过滤
```

## 部署配置

### Docker配置
```dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 环境配置
```yaml
# config.yaml
query_analyzer:
  model:
    provider: "openai"  # 或 "azure", "local"
    model_name: "gpt-4"
    temperature: 0.1
    max_tokens: 1000
  
  cache:
    enabled: true
    max_size: 1000
    ttl: 3600  # 1小时
  
  fallback:
    enabled: true
    confidence_threshold: 0.5
  
  logging:
    level: "INFO"
    format: "json"
```

## 监控指标

### 关键指标
- **分析成功率**: 成功分析的查询比例
- **平均响应时间**: 查询分析的平均耗时
- **缓存命中率**: 缓存使用效率
- **回退使用率**: 回退机制的使用频率
- **分析质量分数**: 基于完整性和一致性的质量评分

### 监控实现
```python
from prometheus_client import Counter, Histogram, Gauge

# 定义指标
analysis_requests = Counter('query_analysis_requests_total', 
                          'Total query analysis requests')
analysis_duration = Histogram('query_analysis_duration_seconds',
                            'Query analysis duration')
analysis_quality = Gauge('query_analysis_quality_score',
                       'Query analysis quality score')

class MonitoredQueryAnalyzer(QueryAnalyzerAgent):
    async def analyze_query(self, query: str) -> Dict[str, Any]:
        analysis_requests.inc()
        
        start_time = time.time()
        try:
            result = await super().analyze_query(query)
            analysis_quality.set(result.get("confidence_score", 0))
            return result
        finally:
            analysis_duration.observe(time.time() - start_time)
```

---

*此文档提供了智能查询分析服务的完整实现指南，包括核心功能、技术架构、代码示例和部署配置。*