<p align="center">
    <img src="https://qianwen-res.oss-accelerate.aliyuncs.com/logo_qwen_embedding.png" width="400"/>
<p>

<p align="center">
   ğŸ¤— <a href="https://huggingface.co/collections/Qwen/qwen3-embedding-6841b2055b99c44d9a4c371f">Huggingface</a> | 
   ğŸ¤– <a href="https://modelscope.cn/collections/Qwen3-Embedding-3edc3762d50f48">ModelScope</a> | 
   ğŸ“ <a href="https://qwenlm.github.io/blog/qwen3-embedding/">æŠ€æœ¯åšå®¢</a> | 
   ğŸ“„ <a href="https://arxiv.org/abs/2506.05176">è®ºæ–‡</a> | 
   ğŸš€ <a href="https://bailian.console.aliyun.com/?tab=model#/model-market/detail/text-embedding-v4">APIæœåŠ¡</a> | 
   ğŸ’¬ <a href="https://discord.gg/yPEP2vHTu4">Discord</a> 
</p>

# Qwen3 Embedding: ä¸‹ä¸€ä»£æ–‡æœ¬åµŒå…¥ä¸é‡æ’åºæ¨¡å‹

## ğŸ“– é¡¹ç›®ç®€ä»‹

Qwen3 Embedding æ˜¯é€šä¹‰åƒé—®å›¢é˜Ÿæ¨å‡ºçš„æœ€æ–°ä¸€ä»£ä¸“ä¸šæ–‡æœ¬åµŒå…¥ä¸é‡æ’åºæ¨¡å‹ç³»åˆ—ï¼Œä¸“é—¨é’ˆå¯¹æ–‡æœ¬åµŒå…¥å’Œæ’åºä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚è¯¥ç³»åˆ—åŸºäº Qwen3 å¯†é›†åŸºç¡€æ¨¡å‹æ„å»ºï¼Œæä¾›äº†å¤šç§è§„æ¨¡ï¼ˆ0.6Bã€4Bã€8Bï¼‰çš„æ–‡æœ¬åµŒå…¥å’Œé‡æ’åºæ¨¡å‹ï¼Œç»§æ‰¿äº†åŸºç¡€æ¨¡å‹å“è¶Šçš„å¤šè¯­è¨€èƒ½åŠ›ã€é•¿æ–‡æœ¬ç†è§£å’Œæ¨ç†æŠ€èƒ½ã€‚

### ğŸŒŸ æ ¸å¿ƒäº®ç‚¹

**ğŸ¯ å“è¶Šæ€§èƒ½**: 
- 8B åµŒå…¥æ¨¡å‹åœ¨ MTEB å¤šè¯­è¨€æ’è¡Œæ¦œä¸Šè·å¾— **ç¬¬1å**ï¼ˆæˆªè‡³2025å¹´6æœˆ5æ—¥ï¼Œå¾—åˆ†**70.58**ï¼‰
- åœ¨æ–‡æœ¬æ£€ç´¢ã€ä»£ç æ£€ç´¢ã€æ–‡æœ¬åˆ†ç±»ã€æ–‡æœ¬èšç±»å’ŒåŒè¯­æŒ–æ˜ç­‰å¤šä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½

**ğŸ”§ çµæ´»å¤šæ ·**:
- æä¾›å®Œæ•´çš„æ¨¡å‹è§„æ¨¡é€‰æ‹©ï¼ˆ0.6B åˆ° 8Bï¼‰
- æ”¯æŒè‡ªå®šä¹‰å‘é‡ç»´åº¦ï¼ˆMRLï¼ŒMatryoshkaè¡¨ç¤ºå­¦ä¹ ï¼‰
- æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰æŒ‡ä»¤ï¼Œå¢å¼ºç‰¹å®šä»»åŠ¡ã€è¯­è¨€æˆ–åœºæ™¯çš„æ€§èƒ½

**ğŸŒ å¤šè¯­è¨€æ”¯æŒ**:
- æ”¯æŒè¶…è¿‡100ç§è¯­è¨€ï¼ŒåŒ…æ‹¬å„ç§ç¼–ç¨‹è¯­è¨€
- æä¾›å¼ºå¤§çš„å¤šè¯­è¨€ã€è·¨è¯­è¨€å’Œä»£ç æ£€ç´¢èƒ½åŠ›

## ğŸ“Š æ¨¡å‹åˆ—è¡¨

| æ¨¡å‹ç±»å‹ | æ¨¡å‹åç§° | å‚æ•°é‡ | å±‚æ•° | åºåˆ—é•¿åº¦ | åµŒå…¥ç»´åº¦ | MRLæ”¯æŒ | æŒ‡ä»¤æ„ŸçŸ¥ |
|---------|---------|------|------|---------|---------|---------|---------|
| æ–‡æœ¬åµŒå…¥ | [Qwen3-Embedding-0.6B](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B) | 0.6B | 28 | 32K | 1024 | âœ… | âœ… |
| æ–‡æœ¬åµŒå…¥ | [Qwen3-Embedding-4B](https://huggingface.co/Qwen/Qwen3-Embedding-4B) | 4B | 36 | 32K | 2560 | âœ… | âœ… |
| æ–‡æœ¬åµŒå…¥ | [Qwen3-Embedding-8B](https://huggingface.co/Qwen/Qwen3-Embedding-8B) | 8B | 36 | 32K | 4096 | âœ… | âœ… |
| æ–‡æœ¬é‡æ’åº | [Qwen3-Reranker-0.6B](https://huggingface.co/Qwen/Qwen3-Reranker-0.6B) | 0.6B | 28 | 32K | - | - | âœ… |
| æ–‡æœ¬é‡æ’åº | [Qwen3-Reranker-4B](https://huggingface.co/Qwen/Qwen3-Reranker-4B) | 4B | 36 | 32K | - | - | âœ… |
| æ–‡æœ¬é‡æ’åº | [Qwen3-Reranker-8B](https://huggingface.co/Qwen/Qwen3-Reranker-8B) | 8B | 36 | 32K | - | - | âœ… |

> **æ³¨æ„**:
> - `MRLæ”¯æŒ` è¡¨ç¤ºåµŒå…¥æ¨¡å‹æ˜¯å¦æ”¯æŒè‡ªå®šä¹‰æœ€ç»ˆåµŒå…¥çš„ç»´åº¦
> - `æŒ‡ä»¤æ„ŸçŸ¥` è¡¨ç¤ºåµŒå…¥æˆ–é‡æ’åºæ¨¡å‹æ˜¯å¦æ”¯æŒæ ¹æ®ä¸åŒä»»åŠ¡è‡ªå®šä¹‰è¾“å…¥æŒ‡ä»¤
> - æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œå¯¹äºå¤§å¤šæ•°ä¸‹æ¸¸ä»»åŠ¡ï¼Œä½¿ç”¨æŒ‡ä»¤é€šå¸¸æ¯”ä¸ä½¿ç”¨æŒ‡ä»¤æé«˜1%åˆ°5%çš„æ€§èƒ½

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

```bash
# å»ºè®®ä½¿ç”¨ transformers>=4.51.0
pip install transformers torch numpy
```

### åµŒå…¥æ¨¡å‹ä½¿ç”¨

#### 1. ä½¿ç”¨ Transformers

```python
import torch
import torch.nn.functional as F
from torch import Tensor
from transformers import AutoTokenizer, AutoModel

def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
    """æœ€åä¸€ä¸ªtokençš„æ± åŒ–æ“ä½œ"""
    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
    if left_padding:
        return last_hidden_states[:, -1]
    else:
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_states.shape[0]
        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]

def get_detailed_instruct(task_description: str, query: str) -> str:
    """æ„å»ºè¯¦ç»†æŒ‡ä»¤"""
    return f'Instruct: {task_description}\nQuery:{query}'

# åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-0.6B', padding_side='left')
model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B')

# å»ºè®®å¯ç”¨ flash_attention_2 ä»¥è·å¾—æ›´å¥½çš„åŠ é€Ÿå’Œå†…å­˜èŠ‚çœ
# model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B', 
#                                  attn_implementation="flash_attention_2", 
#                                  torch_dtype=torch.float16).cuda()

# å®šä¹‰ä»»åŠ¡å’Œæ–‡æœ¬
task = 'Given a web search query, retrieve relevant passages that answer the query'
queries = [
    get_detailed_instruct(task, 'What is the capital of China?'),
    get_detailed_instruct(task, 'Explain gravity')
]
documents = [
    "The capital of China is Beijing.",
    "Gravity is a force that attracts two bodies towards each other."
]

# ç¼–ç 
input_texts = queries + documents
batch_dict = tokenizer(input_texts, padding=True, truncation=True, 
                      max_length=8192, return_tensors="pt")
batch_dict.to(model.device)

outputs = model(**batch_dict)
embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])

# æ ‡å‡†åŒ–å¹¶è®¡ç®—ç›¸ä¼¼åº¦
embeddings = F.normalize(embeddings, p=2, dim=1)
scores = (embeddings[:2] @ embeddings[2:].T)
print(scores.tolist())
```

#### 2. ä½¿ç”¨ vLLMï¼ˆæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰

```python
import torch
import vllm
from vllm import LLM

# åˆå§‹åŒ–æ¨¡å‹
model = LLM(model="Qwen/Qwen3-Embedding-0.6B", task="embed")

# ç¼–ç æ–‡æœ¬
input_texts = [
    'Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery:What is the capital of China?',
    "The capital of China is Beijing."
]

outputs = model.embed(input_texts)
embeddings = torch.tensor([o.outputs.embedding for o in outputs])
scores = (embeddings[:1] @ embeddings[1:].T)
print(scores.tolist())
```

#### 3. ä½¿ç”¨ Sentence Transformers

```python
from sentence_transformers import SentenceTransformer

# åŠ è½½æ¨¡å‹
model = SentenceTransformer("Qwen/Qwen3-Embedding-0.6B")

# ç¼–ç æŸ¥è¯¢å’Œæ–‡æ¡£
queries = ["What is the capital of China?", "Explain gravity"]
documents = [
    "The capital of China is Beijing.",
    "Gravity is a force that attracts two bodies towards each other."
]

# ä½¿ç”¨é¢„å®šä¹‰çš„æŸ¥è¯¢æç¤ºç¼–ç 
query_embeddings = model.encode(queries, prompt_name="query")
document_embeddings = model.encode(documents)

# è®¡ç®—ç›¸ä¼¼åº¦
similarity = model.similarity(query_embeddings, document_embeddings)
print(similarity)
```

### é‡æ’åºæ¨¡å‹ä½¿ç”¨

#### ä½¿ç”¨ Transformers

```python
import torch
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM

def format_instruction(instruction, query, doc):
    """æ ¼å¼åŒ–é‡æ’åºè¾“å…¥"""
    if instruction is None:
        instruction = 'Given a web search query, retrieve relevant passages that answer the query'
    return f"<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}"

# åˆå§‹åŒ–æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-Reranker-0.6B", padding_side='left')
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-Reranker-0.6B").eval()

# è®¾ç½®ç‰¹æ®Štoken
token_false_id = tokenizer.convert_tokens_to_ids("no")
token_true_id = tokenizer.convert_tokens_to_ids("yes")

# å‡†å¤‡è¾“å…¥
task = 'Given a web search query, retrieve relevant passages that answer the query'
queries = ["What is the capital of China?", "Explain gravity"]
documents = [
    "The capital of China is Beijing.",
    "Gravity is a force that attracts two bodies towards each other."
]

pairs = [format_instruction(task, query, doc) for query, doc in zip(queries, documents)]

# è¯¦ç»†çš„å¤„ç†è¿‡ç¨‹è¯·å‚è€ƒ examples/qwen3_reranker_transformers.py
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
Qwen3-Embedding/
â”œâ”€â”€ examples/                          # ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ qwen3_embedding_transformers.py    # åµŒå…¥æ¨¡å‹ Transformers ç¤ºä¾‹
â”‚   â”œâ”€â”€ qwen3_embedding_vllm.py           # åµŒå…¥æ¨¡å‹ vLLM ç¤ºä¾‹
â”‚   â”œâ”€â”€ qwen3_reranker_transformers.py    # é‡æ’åºæ¨¡å‹ Transformers ç¤ºä¾‹
â”‚   â””â”€â”€ qwen3_reranker_vllm.py           # é‡æ’åºæ¨¡å‹ vLLM ç¤ºä¾‹
â”œâ”€â”€ evaluation/                        # è¯„ä¼°è„šæœ¬å’Œå·¥å…·
â”‚   â”œâ”€â”€ run_mteb.py                    # MTEB åµŒå…¥è¯„ä¼°è„šæœ¬
â”‚   â”œâ”€â”€ run_mteb_reranking.py          # MTEB é‡æ’åºè¯„ä¼°è„šæœ¬
â”‚   â”œâ”€â”€ qwen3_embedding_model.py       # åµŒå…¥æ¨¡å‹åŒ…è£…å™¨
â”‚   â”œâ”€â”€ qwen3_reranker_model.py        # é‡æ’åºæ¨¡å‹åŒ…è£…å™¨
â”‚   â””â”€â”€ requirements.txt               # è¯„ä¼°ä¾èµ–
â”œâ”€â”€ qwen3_embedding_technical_report.pdf  # æŠ€æœ¯æŠ¥å‘Š
â””â”€â”€ README.md                          # æœ¬æ–‡æ¡£
```

## ğŸ”§ æ”¯æŒçš„è¯­è¨€

Qwen3-Embedding æ”¯æŒè¶…è¿‡100ç§è¯­è¨€ï¼ŒåŒ…æ‹¬ï¼š

<details>
<summary>ç‚¹å‡»å±•å¼€æ”¯æŒçš„è¯­è¨€åˆ—è¡¨</summary>

| è¯­ç³» | æ”¯æŒçš„è¯­è¨€ |
|-----|----------|
| å°æ¬§è¯­ç³» | è‹±è¯­ã€æ³•è¯­ã€è‘¡è„ç‰™è¯­ã€å¾·è¯­ã€ç½—é©¬å°¼äºšè¯­ã€ç‘å…¸è¯­ã€ä¸¹éº¦è¯­ã€ä¿åŠ åˆ©äºšè¯­ã€ä¿„è¯­ã€æ·å…‹è¯­ã€å¸Œè…Šè¯­ã€ä¹Œå…‹å…°è¯­ã€è¥¿ç­ç‰™è¯­ã€è·å…°è¯­ã€æ–¯æ´›ä¼å…‹è¯­ã€å…‹ç½—åœ°äºšè¯­ã€æ³¢å…°è¯­ã€ç«‹é™¶å®›è¯­ã€æŒªå¨è¯­ã€æ³¢æ–¯è¯­ã€æ–¯æ´›æ–‡å°¼äºšè¯­ã€å¤å‰æ‹‰ç‰¹è¯­ã€æ‹‰è„±ç»´äºšè¯­ã€æ„å¤§åˆ©è¯­ã€å°¼æ³Šå°”è¯­ã€é©¬æ‹‰åœ°è¯­ã€ç™½ä¿„ç½—æ–¯è¯­ã€å¡å°”ç»´äºšè¯­ã€å°åœ°è¯­ã€æ—é®æ™®è¯­ã€å­ŸåŠ æ‹‰è¯­ã€å¥¥é‡Œäºšè¯­ã€å¡”å‰å…‹è¯­ã€æ„ç¬¬ç»ªè¯­ã€çˆ±å°”å…°è¯­ã€æ³•ç½—è¯­ã€ä¿¡å¾·è¯­ã€äºšç¾å°¼äºšè¯­ç­‰ |
| æ±‰è—è¯­ç³» | ä¸­æ–‡ï¼ˆç®€ä½“ä¸­æ–‡ã€ç¹ä½“ä¸­æ–‡ã€ç²¤è¯­ï¼‰ã€ç¼…ç”¸è¯­ |
| é—ªå«è¯­ç³» | é˜¿æ‹‰ä¼¯è¯­ï¼ˆæ ‡å‡†é˜¿æ‹‰ä¼¯è¯­ã€å„æ–¹è¨€ï¼‰ã€å¸Œä¼¯æ¥è¯­ã€é©¬è€³ä»–è¯­ |
| å—å²›è¯­ç³» | å°å°¼è¯­ã€é©¬æ¥è¯­ã€ä»–åŠ ç¦„è¯­ã€å®¿åŠ¡è¯­ã€çˆªå“‡è¯­ã€å·½ä»–è¯­ç­‰ |
| è¾¾ç½—æ¯—è¼è¯­ç³» | æ³°ç±³å°”è¯­ã€æ³°å¢å›ºè¯­ã€å¡çº³è¾¾è¯­ã€é©¬æ‹‰é›…æ‹‰å§†è¯­ |
| çªå¥è¯­ç³» | åœŸè€³å…¶è¯­ã€é˜¿å¡æ‹œç–†è¯­ã€ä¹Œå…¹åˆ«å…‹è¯­ã€å“ˆè¨å…‹è¯­ã€å·´ä»€åŸºå°”è¯­ã€é‘é¼è¯­ |
| å£®ä¾—è¯­ç³» | æ³°è¯­ã€è€æŒè¯­ |
| ä¹Œæ‹‰å°”è¯­ç³» | èŠ¬å…°è¯­ã€çˆ±æ²™å°¼äºšè¯­ã€åŒˆç‰™åˆ©è¯­ |
| å—äºšè¯­ç³» | è¶Šå—è¯­ã€é«˜æ£‰è¯­ |
| å…¶ä»– | æ—¥è¯­ã€éŸ©è¯­ã€æ ¼é²å‰äºšè¯­ã€å·´æ–¯å…‹è¯­ã€æµ·åœ°å…‹é‡Œå¥¥å°”è¯­ã€æ–¯ç“¦å¸Œé‡Œè¯­ç­‰ |

</details>

## ğŸ“ˆ æ€§èƒ½è¯„ä¼°

### MTEB å¤šè¯­è¨€åŸºå‡†æµ‹è¯•

| æ¨¡å‹ | å‚æ•°é‡ | å¹³å‡åˆ†(ä»»åŠ¡) | å¹³å‡åˆ†(ç±»å‹) | åŒè¯­æŒ–æ˜ | åˆ†ç±» | èšç±» | æŒ‡ä»¤æ£€ç´¢ | å¤šè¯­è¨€åˆ†ç±» | é…å¯¹åˆ†ç±» | é‡æ’åº | æ£€ç´¢ | è¯­ä¹‰ç›¸ä¼¼åº¦ |
|-----|-------|------------|------------|----------|------|------|----------|-----------|----------|--------|------|-----------|
| **Qwen3-Embedding-8B** | 8B | **70.58** | **61.69** | **80.89** | **74.00** | **57.65** | 10.06 | 28.66 | **86.40** | **65.63** | **70.88** | **81.08** |
| **Qwen3-Embedding-4B** | 4B | 69.45 | 60.86 | 79.36 | 72.33 | 57.15 | **11.56** | 26.77 | 85.05 | 65.08 | 69.60 | 80.86 |
| **Qwen3-Embedding-0.6B** | 0.6B | 64.33 | 56.00 | 72.22 | 66.83 | 52.33 | 5.09 | 24.59 | 80.83 | 61.41 | 64.64 | 76.17 |

### MTEB è‹±æ–‡åŸºå‡†æµ‹è¯• v2

| æ¨¡å‹ | å‚æ•°é‡ | å¹³å‡åˆ†(ä»»åŠ¡) | å¹³å‡åˆ†(ç±»å‹) | åˆ†ç±» | èšç±» | é…å¯¹åˆ†ç±» | é‡æ’åº | æ£€ç´¢ | è¯­ä¹‰ç›¸ä¼¼åº¦ | æ‘˜è¦ |
|-----|-------|------------|------------|------|------|----------|--------|------|-----------|------|
| **Qwen3-Embedding-8B** | 8B | **75.22** | **68.71** | **90.43** | 58.57 | 87.52 | **51.56** | **69.44** | 88.58 | 34.83 |
| **Qwen3-Embedding-4B** | 4B | 74.60 | 68.10 | 89.84 | 57.51 | 87.01 | 50.76 | 68.46 | **88.72** | 34.39 |
| **Qwen3-Embedding-0.6B** | 0.6B | 70.70 | 64.88 | 85.76 | 54.05 | 84.37 | 48.18 | 61.83 | 86.57 | 33.43 |

### C-MTEB ä¸­æ–‡åŸºå‡†æµ‹è¯•

| æ¨¡å‹ | å‚æ•°é‡ | å¹³å‡åˆ†(ä»»åŠ¡) | å¹³å‡åˆ†(ç±»å‹) | åˆ†ç±» | èšç±» | é…å¯¹åˆ†ç±» | é‡æ’åº | æ£€ç´¢ | è¯­ä¹‰ç›¸ä¼¼åº¦ |
|-----|-------|------------|------------|------|------|----------|--------|------|-----------|
| **Qwen3-Embedding-8B** | 8B | **73.84** | **75.00** | **76.97** | **80.08** | 84.23 | 66.99 | **78.21** | 63.53 |
| **Qwen3-Embedding-4B** | 4B | 72.27 | 73.51 | 75.46 | 77.89 | 83.34 | 66.05 | 77.03 | 61.26 |
| **Qwen3-Embedding-0.6B** | 0.6B | 66.33 | 67.45 | 71.40 | 68.74 | 76.42 | 62.58 | 71.03 | 54.52 |

## ğŸ”¬ è¯„ä¼°å’Œæµ‹è¯•

æœ¬é¡¹ç›®æä¾›äº†å®Œæ•´çš„è¯„ä¼°å·¥å…·æ¥å¤ç°è®ºæ–‡ä¸­çš„ç»“æœã€‚

### è¯„ä¼°åµŒå…¥æ¨¡å‹

```bash
cd evaluation
bash run_mteb.sh ${model_path} ${model_name} ${benchmark_name}
```

å‚æ•°è¯´æ˜ï¼š
- `model_path`: æ¨¡å‹æƒé‡æ–‡ä»¶çš„è·¯å¾„æˆ–åç§°ï¼ˆå¦‚ "Qwen/Qwen3-Embedding-0.6B"ï¼‰
- `model_name`: æ¨¡å‹åç§°ï¼Œç”¨äºå‘½åç»“æœç›®å½•
- `benchmark_name`: åŸºå‡†æµ‹è¯•åç§°ï¼Œæ”¯æŒçš„å€¼ï¼š"MTEB(eng, v2)"ã€"MTEB(cmn, v1)"ã€"MTEB(Code, v1)"ã€"MTEB(Multilingual, v2)"

### è¯„ä¼°é‡æ’åºæ¨¡å‹

```bash
bash run_mteb_reranking.sh ${model_path} ${model_name} ${retrieval_path} ${benchmark}
```

å‚æ•°è¯´æ˜ï¼š
- `model_path`: é‡æ’åºæ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
- `model_name`: æ¨¡å‹åç§°
- `retrieval_path`: åµŒå…¥è¯„ä¼°é˜¶æ®µç”Ÿæˆçš„æ£€ç´¢ç»“æœè·¯å¾„
- `benchmark`: åŸºå‡†æµ‹è¯•åç§°

### æ±‡æ€»è¯„ä¼°ç»“æœ

```bash
python3 summary.py results/${model_name}/${model_name}/no_version_available benchmark_name
```

## ğŸ’¡ ä½¿ç”¨å»ºè®®

1. **æŒ‡ä»¤ä¼˜åŒ–**: å»ºè®®å¼€å‘è€…æ ¹æ®å…·ä½“åœºæ™¯ã€ä»»åŠ¡å’Œè¯­è¨€è‡ªå®šä¹‰æŒ‡ä»¤ï¼Œé€šå¸¸å¯ä»¥å¸¦æ¥1%-5%çš„æ€§èƒ½æå‡
2. **å¤šè¯­è¨€åœºæ™¯**: åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹ï¼Œå»ºè®®ä½¿ç”¨è‹±æ–‡ç¼–å†™æŒ‡ä»¤ï¼Œå› ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­å¤§éƒ¨åˆ†æŒ‡ä»¤éƒ½æ˜¯è‹±æ–‡
3. **æ€§èƒ½ä¼˜åŒ–**: 
   - æ¨èä½¿ç”¨ Flash Attention 2 ä»¥è·å¾—æ›´å¥½çš„åŠ é€Ÿå’Œå†…å­˜èŠ‚çœ
   - ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ vLLM ä»¥è·å¾—æ›´å¥½çš„æ¨ç†æ€§èƒ½
   - å¯ä»¥æ ¹æ®éœ€æ±‚ä½¿ç”¨ MRL åŠŸèƒ½è‡ªå®šä¹‰å‘é‡ç»´åº¦

## ğŸ¤ è´¡çŒ®

æ¬¢è¿ä¸ºé¡¹ç›®è´¡çŒ®ä»£ç å’Œå»ºè®®ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. åˆ›å»º Pull Request

## ğŸ“œ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ä¸åŸå§‹ Qwen é¡¹ç›®ç›¸åŒçš„è®¸å¯è¯ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒæ¨¡å‹é¡µé¢çš„è®¸å¯è¯éƒ¨åˆ†ã€‚

## ğŸ“š å¼•ç”¨

å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ï¼š

```bibtex
@article{qwen3embedding,
  title={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},
  author={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2506.05176},
  year={2025}
}
```
