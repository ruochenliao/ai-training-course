# ä¼ä¸šçº§Agent+RAGçŸ¥è¯†åº“ç³»ç»ŸæŠ€æœ¯è“å›¾

## ğŸ¯ é¡¹ç›®æ„¿æ™¯ä¸æ ¸å¿ƒç›®æ ‡

æ„å»ºä¸€ä¸ªåŸºäºè¯­è¨€å¤§æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆVLMï¼‰çš„ä¸‹ä¸€ä»£æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿä»¥å¤šæ™ºèƒ½ä½“ï¼ˆAgentï¼‰ä¸ºæ ¸å¿ƒåè°ƒå™¨ï¼Œæ·±åº¦èåˆå‘é‡æ£€ç´¢ã€å…³é”®è¯æ£€ç´¢ä¸å›¾è°±æ£€ç´¢ï¼Œä¸ºç”¨æˆ·æä¾›ç²¾å‡†ã€å…¨é¢ã€å¯æº¯æºçš„æ™ºèƒ½é—®ç­”ä½“éªŒã€‚

### æ ¸å¿ƒç‰¹æ€§
- ğŸ¤– **å¤šæ™ºèƒ½ä½“åä½œ**ï¼šåŸºäºAutoGençš„æ™ºèƒ½ä½“ç¼–æ’å’Œåè°ƒ
- ğŸ” **å¤šæ¨¡æ€æ£€ç´¢**ï¼šå‘é‡æ£€ç´¢ + å›¾è°±æ£€ç´¢ + æ··åˆæ£€ç´¢
- ğŸ“Š **æ™ºèƒ½èåˆ**ï¼šå¤šè·¯å¾„æ£€ç´¢ç»“æœçš„æ™ºèƒ½åˆ†æå’Œèåˆ
- ğŸ¨ **ç°ä»£åŒ–ç•Œé¢**ï¼šå‚è€ƒGeminiçš„ç”¨æˆ·ä½“éªŒè®¾è®¡
- ğŸ”’ **ä¼ä¸šçº§å®‰å…¨**ï¼šå®Œæ•´çš„æƒé™ç®¡ç†å’Œæ•°æ®å®‰å…¨ä¿éšœ

## ğŸ—ï¸ æ•´ä½“ç³»ç»Ÿæ¶æ„è®¾è®¡

### ç³»ç»Ÿæ¶æ„å›¾

```mermaid
graph TB
    subgraph "å‰ç«¯å±‚ Frontend Layer"
        UI[ç”¨æˆ·ç•Œé¢ - Next.js/React]
        Admin[ç®¡ç†åå° - Vue3/Nuxt.js]
    end
    
    subgraph "APIç½‘å…³å±‚ API Gateway"
        Gateway[FastAPI Gateway]
        Auth[JWTè®¤è¯]
        RateLimit[é™æµæ§åˆ¶]
    end
    
    subgraph "æ™ºèƒ½ä½“åè°ƒå±‚ Agent Orchestration"
        AutoGen[AutoGenåè°ƒå™¨]
        GraphFlow[Graph Flowå¼•æ“]
        
        subgraph "æ£€ç´¢æ™ºèƒ½ä½“ Retrieval Agents"
            VectorAgent[å‘é‡æ£€ç´¢Agent]
            GraphAgent[å›¾è°±æ£€ç´¢Agent]
            HybridAgent[æ··åˆæ£€ç´¢Agent]
        end
        
        subgraph "å¤„ç†æ™ºèƒ½ä½“ Processing Agents"
            RerankerAgent[é‡æ’Agent]
            FusionAgent[èåˆAgent]
            AnswerAgent[ç­”æ¡ˆç”ŸæˆAgent]
        end
    end
    
    subgraph "æ¨¡å‹æœåŠ¡å±‚ Model Services"
        LLMService[LLMæœåŠ¡ - DeepSeek-Chat]
        VLMService[VLMæœåŠ¡ - Qwen-VL-Max]
        EmbedService[åµŒå…¥æœåŠ¡ - é€šä¹‰åƒé—®3-8B]
        RerankerService[é‡æ’æœåŠ¡ - é€šä¹‰åƒé—®3-Reranker-8B]
    end
    
    subgraph "æ•°æ®å¤„ç†å±‚ Data Processing"
        MarkerService[Markeræ–‡æ¡£è§£æ]
        ChunkService[æ™ºèƒ½åˆ†å—æœåŠ¡]
        GraphExtract[å›¾è°±æŠ½å–æœåŠ¡]
    end
    
    subgraph "å­˜å‚¨å±‚ Storage Layer"
        MySQL[(MySQL - å…³ç³»æ•°æ®)]
        Milvus[(Milvus - å‘é‡æ•°æ®)]
        Neo4j[(Neo4j - å›¾è°±æ•°æ®)]
        MinIO[(MinIO - æ–‡ä»¶å­˜å‚¨)]
    end
    
    UI --> Gateway
    Admin --> Gateway
    Gateway --> AutoGen
    AutoGen --> VectorAgent
    AutoGen --> GraphAgent
    AutoGen --> HybridAgent
    VectorAgent --> Milvus
    GraphAgent --> Neo4j
    HybridAgent --> Milvus
    HybridAgent --> Neo4j
    
    AutoGen --> LLMService
    AutoGen --> VLMService
    VectorAgent --> EmbedService
    RerankerAgent --> RerankerService
    
    MarkerService --> ChunkService
    ChunkService --> Milvus
    GraphExtract --> Neo4j
    
    Gateway --> MySQL
    MarkerService --> MinIO
```

### æ•°æ®æµå›¾

#### 1. æ•°æ®å…¥åº“æµç¨‹ï¼ˆIngestion Pipelineï¼‰

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Admin as ç®¡ç†åå°
    participant Marker as MarkeræœåŠ¡
    participant Chunk as åˆ†å—æœåŠ¡
    participant Embed as åµŒå…¥æœåŠ¡
    participant Graph as å›¾è°±æŠ½å–
    participant Milvus as Milvus
    participant Neo4j as Neo4j
    participant MySQL as MySQL
    
    User->>Admin: ä¸Šä¼ æ–‡æ¡£
    Admin->>Marker: æ–‡æ¡£è§£æè¯·æ±‚
    Marker->>Marker: è§£ææ–‡æ¡£(è¡¨æ ¼/å›¾ç‰‡)
    Marker->>Chunk: è¿”å›ç»“æ„åŒ–å†…å®¹
    
    par å¹¶è¡Œå¤„ç†
        Chunk->>Embed: æ–‡æœ¬å‘é‡åŒ–
        Embed->>Milvus: å­˜å‚¨å‘é‡
        and
        Chunk->>Graph: å®ä½“å…³ç³»æŠ½å–
        Graph->>Neo4j: å­˜å‚¨å›¾è°±
    end
    
    Chunk->>MySQL: å­˜å‚¨å…ƒæ•°æ®
    Admin->>User: å¤„ç†å®Œæˆé€šçŸ¥
```

#### 2. æŸ¥è¯¢å¤„ç†æµç¨‹ï¼ˆQuery Pipelineï¼‰

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant UI as å¯¹è¯ç•Œé¢
    participant AutoGen as AutoGenåè°ƒå™¨
    participant VAgent as å‘é‡æ£€ç´¢Agent
    participant GAgent as å›¾è°±æ£€ç´¢Agent
    participant Fusion as èåˆAgent
    participant LLM as LLMæœåŠ¡
    
    User->>UI: è¾“å…¥é—®é¢˜
    UI->>AutoGen: å‘é€æŸ¥è¯¢è¯·æ±‚
    
    AutoGen->>AutoGen: åˆ†ææ£€ç´¢ç­–ç•¥
    
    par å¹¶è¡Œæ£€ç´¢
        AutoGen->>VAgent: è¯­ä¹‰æ£€ç´¢
        VAgent->>VAgent: MilvusæŸ¥è¯¢+é‡æ’
        and
        AutoGen->>GAgent: å›¾è°±æ£€ç´¢
        GAgent->>GAgent: CypheræŸ¥è¯¢
    end
    
    VAgent->>Fusion: è¿”å›å‘é‡ç»“æœ
    GAgent->>Fusion: è¿”å›å›¾è°±ç»“æœ
    
    Fusion->>Fusion: ç»“æœåˆ†æä¸èåˆ
    Fusion->>LLM: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    LLM->>UI: æµå¼è¿”å›ç­”æ¡ˆ
    UI->>User: æ˜¾ç¤ºç­”æ¡ˆå’Œæ¥æº
```

## ğŸ› ï¸ æŠ€æœ¯æ ˆé€‰å‹

### åç«¯æŠ€æœ¯æ ˆ
- **è¯­è¨€**: Python 3.10+
- **Webæ¡†æ¶**: FastAPI (é«˜æ€§èƒ½å¼‚æ­¥æ¡†æ¶)
- **æ™ºèƒ½ä½“æ¡†æ¶**: AutoGen (ConversableAgent + Graph Flow)
- **ORM**: Tortoise ORM (å¼‚æ­¥ORM)
- **ä»»åŠ¡é˜Ÿåˆ—**: Celery + Redis
- **ç¼“å­˜**: Redis
- **æ—¥å¿—**: Loguru
- **ç›‘æ§**: Prometheus + Grafana

### å‰ç«¯æŠ€æœ¯æ ˆ
- **è¯­è¨€**: TypeScript
- **æ¡†æ¶**: 
  - ç”¨æˆ·ç«¯: Next.js 14 + React 18
  - ç®¡ç†ç«¯: Vue 3 + Nuxt.js 3
- **UIç»„ä»¶åº“**: 
  - React: Ant Design + Tailwind CSS
  - Vue: Naive UI + UnoCSS
- **çŠ¶æ€ç®¡ç†**: 
  - React: Zustand
  - Vue: Pinia
- **å›¾è¡¨åº“**: ECharts + D3.js
- **Markdown**: ReactMarkdown + Prism

### AIæ¨¡å‹æœåŠ¡
- **LLM**: DeepSeek-Chat (ä¸»è¦æ¨ç†æ¨¡å‹)
- **VLM**: Qwen-VL-Max (å¤šæ¨¡æ€ç†è§£)
- **åµŒå…¥æ¨¡å‹**: é€šä¹‰åƒé—®3-8B (å‘é‡åŒ–)
- **é‡æ’æ¨¡å‹**: é€šä¹‰åƒé—®3-Reranker-8B (ç»“æœé‡æ’)
- **éƒ¨ç½²æ–¹å¼**: ModelScopeæœ¬åœ°åŒ–éƒ¨ç½²

### æ•°æ®åº“é€‰å‹
- **å…³ç³»å‹æ•°æ®åº“**: MySQL 8.0
  - ç”¨æˆ·ä¿¡æ¯ã€æƒé™ç®¡ç†
  - çŸ¥è¯†åº“å…ƒæ•°æ®
  - ä¼šè¯è®°å½•
- **å‘é‡æ•°æ®åº“**: Milvus 2.4+
  - æ–‡æ¡£å‘é‡å­˜å‚¨
  - æ··åˆæ£€ç´¢æ”¯æŒ
- **å›¾æ•°æ®åº“**: Neo4j 5.x
  - å®ä½“å…³ç³»å­˜å‚¨
  - çŸ¥è¯†å›¾è°±æŸ¥è¯¢
- **æ–‡ä»¶å­˜å‚¨**: MinIO
  - åŸå§‹æ–‡æ¡£å­˜å‚¨
  - å›¾ç‰‡èµ„æºå­˜å‚¨

### æ–‡æ¡£å¤„ç†
- **è§£æå¼•æ“**: Marker (äºŒæ¬¡å°è£…)
  - æ”¯æŒPDFã€Wordã€PPTç­‰æ ¼å¼
  - è¡¨æ ¼å’Œå›¾ç‰‡æ™ºèƒ½è¯†åˆ«
  - ç»“æ„åŒ–å†…å®¹æå–

## ğŸ“Š æ•°æ®åº“è®¾è®¡

### MySQLæ ¸å¿ƒè¡¨ç»“æ„

```sql
-- ç”¨æˆ·è¡¨
CREATE TABLE users (
    id INT PRIMARY KEY AUTO_INCREMENT,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    is_superuser BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);

-- çŸ¥è¯†åº“è¡¨
CREATE TABLE knowledge_bases (
    id INT PRIMARY KEY AUTO_INCREMENT,
    name VARCHAR(100) NOT NULL,
    description TEXT,
    knowledge_type ENUM('customer_service', 'text_sql', 'rag', 'content_creation') DEFAULT 'rag',
    is_public BOOLEAN DEFAULT FALSE,
    owner_id INT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    FOREIGN KEY (owner_id) REFERENCES users(id) ON DELETE CASCADE,
    INDEX idx_owner_type (owner_id, knowledge_type),
    INDEX idx_public (is_public)
);

-- æ–‡æ¡£è¡¨
CREATE TABLE documents (
    id INT PRIMARY KEY AUTO_INCREMENT,
    name VARCHAR(255) NOT NULL,
    file_path VARCHAR(500) NOT NULL,
    file_size BIGINT NOT NULL,
    file_type VARCHAR(100) NOT NULL,
    processing_status ENUM('pending', 'processing', 'completed', 'failed') DEFAULT 'pending',
    processing_error TEXT,
    knowledge_base_id INT NOT NULL,
    uploaded_by INT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    FOREIGN KEY (knowledge_base_id) REFERENCES knowledge_bases(id) ON DELETE CASCADE,
    FOREIGN KEY (uploaded_by) REFERENCES users(id),
    INDEX idx_kb_status (knowledge_base_id, processing_status)
);

-- ä¼šè¯è¡¨
CREATE TABLE conversations (
    id INT PRIMARY KEY AUTO_INCREMENT,
    user_id INT NOT NULL,
    title VARCHAR(200),
    knowledge_base_ids JSON, -- ä½¿ç”¨çš„çŸ¥è¯†åº“IDåˆ—è¡¨
    retrieval_strategy JSON, -- æ£€ç´¢ç­–ç•¥é…ç½®
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE,
    INDEX idx_user_created (user_id, created_at)
);

-- æ¶ˆæ¯è¡¨
CREATE TABLE messages (
    id INT PRIMARY KEY AUTO_INCREMENT,
    conversation_id INT NOT NULL,
    role ENUM('user', 'assistant') NOT NULL,
    content TEXT NOT NULL,
    metadata JSON, -- å­˜å‚¨æ¥æºä¿¡æ¯ã€æ£€ç´¢ç»“æœç­‰
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE,
    INDEX idx_conversation_created (conversation_id, created_at)
);
```

### Milvusé›†åˆè®¾è®¡

```python
# å‘é‡é›†åˆSchema
collection_schema = {
    "collection_name": "knowledge_vectors",
    "fields": [
        {"name": "id", "type": "varchar", "max_length": 100, "is_primary": True},
        {"name": "vector", "type": "float_vector", "dim": 1024},  # åµŒå…¥ç»´åº¦
        {"name": "knowledge_base_id", "type": "int64"},
        {"name": "document_id", "type": "int64"},
        {"name": "chunk_index", "type": "int64"},
        {"name": "content", "type": "varchar", "max_length": 65535},
        {"name": "metadata", "type": "json"}
    ],
    "indexes": [
        {"field": "vector", "index_type": "HNSW", "metric_type": "COSINE"},
        {"field": "knowledge_base_id", "index_type": "STL_SORT"},
        {"field": "document_id", "index_type": "STL_SORT"}
    ]
}
```

### Neo4jå›¾è°±è®¾è®¡

```cypher
// å®ä½“èŠ‚ç‚¹
CREATE CONSTRAINT entity_id IF NOT EXISTS FOR (e:Entity) REQUIRE e.id IS UNIQUE;

// æ–‡æ¡£èŠ‚ç‚¹
CREATE CONSTRAINT document_id IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE;

// çŸ¥è¯†åº“èŠ‚ç‚¹
CREATE CONSTRAINT kb_id IF NOT EXISTS FOR (k:KnowledgeBase) REQUIRE k.id IS UNIQUE;

// å…³ç³»ç±»å‹
// (:Entity)-[:RELATED_TO]->(:Entity)
// (:Entity)-[:MENTIONED_IN]->(:Document)
// (:Document)-[:BELONGS_TO]->(:KnowledgeBase)
```

## ğŸ”§ APIç«¯ç‚¹è®¾è®¡

### æ ¸å¿ƒAPIç«¯ç‚¹

```python
# ç”¨æˆ·è®¤è¯
POST   /api/v1/auth/login
POST   /api/v1/auth/logout
POST   /api/v1/auth/refresh
GET    /api/v1/auth/me

# çŸ¥è¯†åº“ç®¡ç†
GET    /api/v1/knowledge-bases/
POST   /api/v1/knowledge-bases/
GET    /api/v1/knowledge-bases/{id}
PUT    /api/v1/knowledge-bases/{id}
DELETE /api/v1/knowledge-bases/{id}

# æ–‡æ¡£ç®¡ç†
POST   /api/v1/knowledge-bases/{kb_id}/documents/upload
GET    /api/v1/knowledge-bases/{kb_id}/documents/
GET    /api/v1/documents/{id}
DELETE /api/v1/documents/{id}
GET    /api/v1/documents/{id}/chunks

# æ™ºèƒ½å¯¹è¯
POST   /api/v1/chat/conversations/
GET    /api/v1/chat/conversations/
GET    /api/v1/chat/conversations/{id}
DELETE /api/v1/chat/conversations/{id}
POST   /api/v1/chat/conversations/{id}/messages
GET    /api/v1/chat/conversations/{id}/messages
POST   /api/v1/chat/stream  # WebSocketæµå¼å¯¹è¯

# æ£€ç´¢æœåŠ¡
POST   /api/v1/search/vector
POST   /api/v1/search/graph
POST   /api/v1/search/hybrid

# å›¾è°±å¯è§†åŒ–
GET    /api/v1/graph/entities
GET    /api/v1/graph/relationships
GET    /api/v1/graph/subgraph/{entity_id}
POST   /api/v1/graph/query  # CypheræŸ¥è¯¢

# ç³»ç»Ÿç®¡ç†
GET    /api/v1/admin/stats
GET    /api/v1/admin/health
GET    /api/v1/admin/logs
```

## ğŸ¨ å‰ç«¯ç•Œé¢è®¾è®¡

### ç”¨æˆ·å¯¹è¯ç•Œé¢ï¼ˆå‚è€ƒGeminiè®¾è®¡ï¼‰

#### å¸ƒå±€ç»“æ„
```typescript
interface ChatLayoutProps {
  sidebar: {
    conversations: Conversation[];
    knowledgeBases: KnowledgeBase[];
    searchHistory: SearchHistory[];
  };
  mainArea: {
    messages: Message[];
    inputArea: ChatInput;
    retrievalSettings: RetrievalSettings;
  };
  rightPanel?: {
    sources: SourceReference[];
    graphVisualization: GraphView;
  };
}

// æ£€ç´¢ç­–ç•¥é€‰æ‹©å™¨
interface RetrievalSettings {
  semanticSearch: boolean;
  hybridSearch: boolean;
  graphSearch: boolean;
  knowledgeBaseIds: number[];
  searchDepth: number;
  maxResults: number;
}
```

#### æ ¸å¿ƒç»„ä»¶è®¾è®¡
```typescript
// æ¶ˆæ¯ç»„ä»¶
const MessageComponent: React.FC<{
  message: Message;
  sources?: SourceReference[];
}> = ({ message, sources }) => {
  return (
    <div className="message-container">
      <div className="message-content">
        <ReactMarkdown
          components={{
            code: CodeBlock,
            table: TableComponent,
            img: ImageComponent
          }}
        >
          {message.content}
        </ReactMarkdown>
      </div>
      {sources && (
        <SourcePanel sources={sources} />
      )}
    </div>
  );
};

// æµå¼è¾“å…¥ç»„ä»¶
const StreamingInput: React.FC = () => {
  const [isStreaming, setIsStreaming] = useState(false);
  const [streamContent, setStreamContent] = useState('');

  const handleStream = useCallback(async (query: string) => {
    setIsStreaming(true);
    const response = await fetch('/api/v1/chat/stream', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ query, settings: retrievalSettings })
    });

    const reader = response.body?.getReader();
    // å¤„ç†æµå¼å“åº”...
  }, [retrievalSettings]);

  return (
    <div className="streaming-input">
      <textarea
        placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."
        onKeyDown={handleKeyDown}
      />
      <RetrievalSettingsPanel />
      <button onClick={() => handleStream(inputValue)}>
        å‘é€
      </button>
    </div>
  );
};
```

### ç®¡ç†åå°ç•Œé¢

#### çŸ¥è¯†åº“ç®¡ç†
```vue
<template>
  <div class="knowledge-base-management">
    <!-- çŸ¥è¯†åº“åˆ—è¡¨ -->
    <n-data-table
      :columns="columns"
      :data="knowledgeBases"
      :pagination="pagination"
      :loading="loading"
    />

    <!-- æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ -->
    <n-upload
      multiple
      directory-dnd
      :custom-request="handleFileUpload"
      :show-file-list="false"
    >
      <n-upload-dragger>
        <div>
          <n-icon size="48" :depth="3">
            <CloudUploadOutline />
          </n-icon>
        </div>
        <n-text style="font-size: 16px">
          ç‚¹å‡»æˆ–è€…æ‹–åŠ¨æ–‡ä»¶åˆ°è¯¥åŒºåŸŸæ¥ä¸Šä¼ 
        </n-text>
        <n-p depth="3" style="margin: 8px 0 0 0">
          æ”¯æŒ PDFã€Wordã€PPTã€Markdown ç­‰æ ¼å¼
        </n-p>
      </n-upload-dragger>
    </n-upload>

    <!-- å¤„ç†è¿›åº¦ç›‘æ§ -->
    <ProcessingMonitor :files="processingFiles" />
  </div>
</template>

<script setup lang="ts">
import { ref, onMounted } from 'vue';
import { useKnowledgeBaseStore } from '@/stores/knowledgeBase';

const knowledgeBaseStore = useKnowledgeBaseStore();
const loading = ref(false);
const processingFiles = ref<ProcessingFile[]>([]);

const handleFileUpload = async (options: UploadCustomRequestOptions) => {
  const { file, onProgress, onFinish, onError } = options;

  try {
    const formData = new FormData();
    formData.append('file', file.file as File);
    formData.append('knowledge_base_id', selectedKnowledgeBaseId.value);

    const response = await fetch('/api/v1/knowledge-bases/upload', {
      method: 'POST',
      body: formData,
      onUploadProgress: (progressEvent) => {
        const progress = Math.round(
          (progressEvent.loaded * 100) / progressEvent.total
        );
        onProgress({ percent: progress });
      }
    });

    if (response.ok) {
      onFinish();
      // å¼€å§‹ç›‘æ§å¤„ç†è¿›åº¦
      startProcessingMonitor(file.id);
    } else {
      onError();
    }
  } catch (error) {
    onError();
  }
};
</script>
```

#### å›¾è°±å¯è§†åŒ–ç»„ä»¶
```typescript
// ä½¿ç”¨EChartså®ç°å›¾è°±å¯è§†åŒ–
const GraphVisualization: React.FC<{
  graphData: GraphData;
  onNodeClick: (node: GraphNode) => void;
}> = ({ graphData, onNodeClick }) => {
  const chartRef = useRef<HTMLDivElement>(null);
  const [chart, setChart] = useState<echarts.ECharts>();

  useEffect(() => {
    if (chartRef.current) {
      const chartInstance = echarts.init(chartRef.current);

      const option: echarts.EChartsOption = {
        title: {
          text: 'çŸ¥è¯†å›¾è°±',
          left: 'center'
        },
        tooltip: {
          trigger: 'item',
          formatter: (params: any) => {
            if (params.dataType === 'node') {
              return `å®ä½“: ${params.data.name}<br/>ç±»å‹: ${params.data.category}`;
            } else {
              return `å…³ç³»: ${params.data.name}`;
            }
          }
        },
        series: [{
          type: 'graph',
          layout: 'force',
          data: graphData.nodes.map(node => ({
            id: node.id,
            name: node.name,
            category: node.type,
            symbolSize: node.importance * 20,
            itemStyle: {
              color: getNodeColor(node.type)
            }
          })),
          links: graphData.edges.map(edge => ({
            source: edge.source,
            target: edge.target,
            name: edge.relationship,
            lineStyle: {
              color: getEdgeColor(edge.relationship)
            }
          })),
          categories: getCategories(graphData.nodes),
          roam: true,
          force: {
            repulsion: 1000,
            edgeLength: 100
          },
          emphasis: {
            focus: 'adjacency'
          }
        }]
      };

      chartInstance.setOption(option);
      chartInstance.on('click', (params) => {
        if (params.dataType === 'node') {
          onNodeClick(params.data);
        }
      });

      setChart(chartInstance);
    }
  }, [graphData, onNodeClick]);

  return (
    <div
      ref={chartRef}
      style={{ width: '100%', height: '600px' }}
    />
  );
};
```

## ğŸ¤– AutoGenæ™ºèƒ½ä½“æ¶æ„è®¾è®¡

### æ™ºèƒ½ä½“å®šä¹‰
```python
from autogen import ConversableAgent, GroupChat, GroupChatManager
from autogen.agentchat.contrib.graph_flow import GraphFlow

class RAGOrchestrator:
    """RAGç³»ç»Ÿçš„ä¸»åè°ƒå™¨"""

    def __init__(self, config: RAGConfig):
        self.config = config
        self.agents = self._initialize_agents()
        self.graph_flow = self._setup_graph_flow()

    def _initialize_agents(self) -> Dict[str, ConversableAgent]:
        """åˆå§‹åŒ–æ‰€æœ‰æ™ºèƒ½ä½“"""

        # æŸ¥è¯¢åˆ†ææ™ºèƒ½ä½“
        query_analyzer = ConversableAgent(
            name="query_analyzer",
            system_message="""ä½ æ˜¯æŸ¥è¯¢åˆ†æä¸“å®¶ã€‚åˆ†æç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾ã€å®ä½“å’Œæ£€ç´¢éœ€æ±‚ã€‚
            è¾“å‡ºæ ¼å¼ï¼š
            {
                "intent": "é—®ç­”/æœç´¢/åˆ†æ",
                "entities": ["å®ä½“1", "å®ä½“2"],
                "query_type": "factual/analytical/procedural",
                "retrieval_strategy": ["vector", "graph", "hybrid"]
            }""",
            llm_config=self.config.llm_config
        )

        # å‘é‡æ£€ç´¢æ™ºèƒ½ä½“
        vector_retriever = ConversableAgent(
            name="vector_retriever",
            system_message="""ä½ æ˜¯å‘é‡æ£€ç´¢ä¸“å®¶ã€‚åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µã€‚
            ä½¿ç”¨å·¥å…·ï¼švector_search, rerank_results""",
            llm_config=self.config.llm_config,
            function_map={
                "vector_search": self._vector_search,
                "rerank_results": self._rerank_results
            }
        )

        # å›¾è°±æ£€ç´¢æ™ºèƒ½ä½“
        graph_retriever = ConversableAgent(
            name="graph_retriever",
            system_message="""ä½ æ˜¯çŸ¥è¯†å›¾è°±æ£€ç´¢ä¸“å®¶ã€‚é€šè¿‡å®ä½“å…³ç³»æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯ã€‚
            ä½¿ç”¨å·¥å…·ï¼šentity_search, relationship_query, subgraph_extraction""",
            llm_config=self.config.llm_config,
            function_map={
                "entity_search": self._entity_search,
                "relationship_query": self._relationship_query,
                "subgraph_extraction": self._subgraph_extraction
            }
        )

        # ç»“æœèåˆæ™ºèƒ½ä½“
        result_fusion = ConversableAgent(
            name="result_fusion",
            system_message="""ä½ æ˜¯ç»“æœèåˆä¸“å®¶ã€‚åˆ†æå’Œèåˆæ¥è‡ªä¸åŒæ£€ç´¢è·¯å¾„çš„ç»“æœã€‚
            ä»»åŠ¡ï¼š
            1. å»é‡å’Œä¸€è‡´æ€§æ£€æŸ¥
            2. é‡è¦æ€§è¯„åˆ†
            3. äº’è¡¥æ€§åˆ†æ
            4. ç”Ÿæˆèåˆåçš„ä¸Šä¸‹æ–‡""",
            llm_config=self.config.llm_config
        )

        # ç­”æ¡ˆç”Ÿæˆæ™ºèƒ½ä½“
        answer_generator = ConversableAgent(
            name="answer_generator",
            system_message="""ä½ æ˜¯ç­”æ¡ˆç”Ÿæˆä¸“å®¶ã€‚åŸºäºèåˆåçš„ä¸Šä¸‹æ–‡ç”Ÿæˆå‡†ç¡®ã€å…¨é¢çš„ç­”æ¡ˆã€‚
            è¦æ±‚ï¼š
            1. ç­”æ¡ˆå‡†ç¡®ä¸”æœ‰ä¾æ®
            2. æä¾›æ˜ç¡®çš„æ¥æºå¼•ç”¨
            3. ç»“æ„åŒ–å’Œæ˜“è¯»æ€§
            4. å¤„ç†å¤šæ¨¡æ€å†…å®¹""",
            llm_config=self.config.llm_config
        )

        return {
            "query_analyzer": query_analyzer,
            "vector_retriever": vector_retriever,
            "graph_retriever": graph_retriever,
            "result_fusion": result_fusion,
            "answer_generator": answer_generator
        }

    def _setup_graph_flow(self) -> GraphFlow:
        """è®¾ç½®æ™ºèƒ½ä½“åä½œæµç¨‹"""

        # å®šä¹‰å·¥ä½œæµå›¾
        workflow = {
            "query_analyzer": {
                "next": ["vector_retriever", "graph_retriever"],
                "condition": "parallel"
            },
            "vector_retriever": {
                "next": ["result_fusion"],
                "condition": "always"
            },
            "graph_retriever": {
                "next": ["result_fusion"],
                "condition": "always"
            },
            "result_fusion": {
                "next": ["answer_generator"],
                "condition": "always"
            },
            "answer_generator": {
                "next": [],
                "condition": "terminal"
            }
        }

        return GraphFlow(
            agents=list(self.agents.values()),
            workflow=workflow,
            max_rounds=10
        )

    async def process_query(self, query: str, context: Dict) -> Dict:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""

        # å¯åŠ¨å·¥ä½œæµ
        result = await self.graph_flow.run(
            initial_message=query,
            context=context
        )

        return {
            "answer": result.final_answer,
            "sources": result.sources,
            "reasoning": result.reasoning_chain,
            "confidence": result.confidence_score
        }
```

### æ£€ç´¢å·¥å…·å®ç°
```python
class RetrievalTools:
    """æ£€ç´¢å·¥å…·é›†åˆ"""

    def __init__(self, milvus_client, neo4j_client, reranker_service):
        self.milvus = milvus_client
        self.neo4j = neo4j_client
        self.reranker = reranker_service

    async def vector_search(self, query: str, kb_ids: List[int], limit: int = 10) -> List[Dict]:
        """å‘é‡æ£€ç´¢"""
        # æŸ¥è¯¢å‘é‡åŒ–
        query_vector = await self.embedding_service.embed(query)

        # Milvusæ£€ç´¢
        search_params = {
            "metric_type": "COSINE",
            "params": {"nprobe": 16}
        }

        results = self.milvus.search(
            collection_name="knowledge_vectors",
            data=[query_vector],
            anns_field="vector",
            param=search_params,
            limit=limit,
            expr=f"knowledge_base_id in {kb_ids}"
        )

        return [
            {
                "id": hit.id,
                "content": hit.entity.get("content"),
                "score": hit.score,
                "metadata": hit.entity.get("metadata"),
                "source": "vector"
            }
            for hit in results[0]
        ]

    async def graph_search(self, entities: List[str], kb_ids: List[int]) -> List[Dict]:
        """å›¾è°±æ£€ç´¢"""
        cypher_query = """
        MATCH (e:Entity)-[r]->(related:Entity)
        WHERE e.name IN $entities
        AND e.knowledge_base_id IN $kb_ids
        RETURN e, r, related,
               e.name as entity_name,
               type(r) as relationship,
               related.name as related_entity,
               r.confidence as confidence
        ORDER BY r.confidence DESC
        LIMIT 50
        """

        results = self.neo4j.run(
            cypher_query,
            entities=entities,
            kb_ids=kb_ids
        )

        return [
            {
                "entity": record["entity_name"],
                "relationship": record["relationship"],
                "related_entity": record["related_entity"],
                "confidence": record["confidence"],
                "source": "graph"
            }
            for record in results
        ]

    async def rerank_results(self, query: str, results: List[Dict]) -> List[Dict]:
        """ç»“æœé‡æ’"""
        if not results:
            return results

        # å‡†å¤‡é‡æ’è¾“å…¥
        passages = [result["content"] for result in results]

        # è°ƒç”¨é‡æ’æœåŠ¡
        scores = await self.reranker.rerank(
            query=query,
            passages=passages
        )

        # æ›´æ–°åˆ†æ•°å¹¶é‡æ–°æ’åº
        for i, result in enumerate(results):
            result["rerank_score"] = scores[i]

        return sorted(results, key=lambda x: x["rerank_score"], reverse=True)
```

## ğŸ“„ æ–‡æ¡£å¤„ç†æœåŠ¡è®¾è®¡

### MarkeræœåŠ¡å°è£…
```python
class MarkerDocumentProcessor:
    """Markeræ–‡æ¡£å¤„ç†æœåŠ¡å°è£…"""

    def __init__(self, config: MarkerConfig):
        self.config = config
        self.supported_formats = {
            '.pdf': self._process_pdf,
            '.docx': self._process_docx,
            '.pptx': self._process_pptx,
            '.md': self._process_markdown,
            '.txt': self._process_text
        }

    async def process_document(self, file_path: str, file_type: str) -> ProcessingResult:
        """å¤„ç†æ–‡æ¡£å¹¶è¿”å›ç»“æ„åŒ–å†…å®¹"""

        try:
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å¤„ç†æ–¹æ³•
            processor = self.supported_formats.get(file_type.lower())
            if not processor:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")

            # æ‰§è¡Œå¤„ç†
            result = await processor(file_path)

            # åå¤„ç†ï¼šæ¸…ç†å’Œä¼˜åŒ–
            result = await self._post_process(result)

            return ProcessingResult(
                success=True,
                content=result.content,
                metadata=result.metadata,
                tables=result.tables,
                images=result.images,
                processing_time=result.processing_time
            )

        except Exception as e:
            logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return ProcessingResult(
                success=False,
                error=str(e)
            )

    async def _process_pdf(self, file_path: str) -> RawProcessingResult:
        """å¤„ç†PDFæ–‡ä»¶"""
        from marker.convert import convert_single_pdf
        from marker.models import load_all_models

        # åŠ è½½æ¨¡å‹ï¼ˆå¯ä»¥ç¼“å­˜ä»¥æé«˜æ€§èƒ½ï¼‰
        model_lst = load_all_models()

        # è½¬æ¢PDF
        full_text, images, out_meta = convert_single_pdf(
            fname=file_path,
            model_lst=model_lst,
            max_pages=self.config.max_pages,
            langs=self.config.languages
        )

        # æå–è¡¨æ ¼
        tables = self._extract_tables_from_markdown(full_text)

        return RawProcessingResult(
            content=full_text,
            metadata=out_meta,
            tables=tables,
            images=images,
            processing_time=time.time() - start_time
        )

    async def _extract_tables_from_markdown(self, markdown_text: str) -> List[Table]:
        """ä»Markdownä¸­æå–è¡¨æ ¼"""
        tables = []
        table_pattern = r'\|.*?\|.*?\n(?:\|.*?\|.*?\n)*'

        for match in re.finditer(table_pattern, markdown_text, re.MULTILINE):
            table_md = match.group()
            table_data = self._parse_markdown_table(table_md)

            tables.append(Table(
                content=table_md,
                data=table_data,
                position=match.span()
            ))

        return tables

    async def _post_process(self, result: RawProcessingResult) -> RawProcessingResult:
        """åå¤„ç†ï¼šæ¸…ç†å’Œä¼˜åŒ–å†…å®¹"""

        # æ¸…ç†æ–‡æœ¬
        cleaned_content = self._clean_text(result.content)

        # ä¼˜åŒ–è¡¨æ ¼æ ¼å¼
        optimized_tables = [
            self._optimize_table(table) for table in result.tables
        ]

        # å¤„ç†å›¾ç‰‡
        processed_images = await self._process_images(result.images)

        return RawProcessingResult(
            content=cleaned_content,
            metadata=result.metadata,
            tables=optimized_tables,
            images=processed_images,
            processing_time=result.processing_time
        )

class DocumentChunker:
    """æ™ºèƒ½æ–‡æ¡£åˆ†å—å™¨"""

    def __init__(self, config: ChunkingConfig):
        self.config = config
        self.text_splitter = self._initialize_text_splitter()

    def _initialize_text_splitter(self):
        """åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨"""
        if self.config.chunking_strategy == "semantic":
            from langchain.text_splitter import SemanticChunker
            return SemanticChunker(
                embeddings=self.config.embedding_model,
                breakpoint_threshold_type="percentile"
            )
        elif self.config.chunking_strategy == "recursive":
            from langchain.text_splitter import RecursiveCharacterTextSplitter
            return RecursiveCharacterTextSplitter(
                chunk_size=self.config.chunk_size,
                chunk_overlap=self.config.chunk_overlap,
                separators=["\n\n", "\n", " ", ""]
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†å—ç­–ç•¥: {self.config.chunking_strategy}")

    async def chunk_document(self, content: str, metadata: Dict) -> List[DocumentChunk]:
        """å¯¹æ–‡æ¡£è¿›è¡Œæ™ºèƒ½åˆ†å—"""

        # é¢„å¤„ç†ï¼šä¿æŠ¤ç‰¹æ®Šå—
        protected_blocks = self._identify_protected_blocks(content)
        processed_content = self._protect_special_blocks(content, protected_blocks)

        # æ‰§è¡Œåˆ†å—
        chunks = self.text_splitter.split_text(processed_content)

        # åå¤„ç†ï¼šæ¢å¤ç‰¹æ®Šå—
        processed_chunks = []
        for i, chunk in enumerate(chunks):
            restored_chunk = self._restore_special_blocks(chunk, protected_blocks)

            chunk_metadata = {
                **metadata,
                "chunk_index": i,
                "total_chunks": len(chunks),
                "chunk_size": len(restored_chunk),
                "chunk_type": self._classify_chunk_type(restored_chunk)
            }

            processed_chunks.append(DocumentChunk(
                content=restored_chunk,
                metadata=chunk_metadata,
                embedding=None  # å°†åœ¨åç»­æ­¥éª¤ä¸­ç”Ÿæˆ
            ))

        return processed_chunks

    def _identify_protected_blocks(self, content: str) -> List[ProtectedBlock]:
        """è¯†åˆ«éœ€è¦ä¿æŠ¤çš„ç‰¹æ®Šå—ï¼ˆä»£ç ã€è¡¨æ ¼ã€å…¬å¼ç­‰ï¼‰"""
        protected_blocks = []

        # ä»£ç å—
        code_pattern = r'```[\s\S]*?```'
        for match in re.finditer(code_pattern, content):
            protected_blocks.append(ProtectedBlock(
                type="code",
                content=match.group(),
                start=match.start(),
                end=match.end()
            ))

        # è¡¨æ ¼
        table_pattern = r'\|.*?\|.*?\n(?:\|.*?\|.*?\n)*'
        for match in re.finditer(table_pattern, content, re.MULTILINE):
            protected_blocks.append(ProtectedBlock(
                type="table",
                content=match.group(),
                start=match.start(),
                end=match.end()
            ))

        # LaTeXå…¬å¼
        latex_pattern = r'\$\$[\s\S]*?\$\$|\$[^$]*?\$'
        for match in re.finditer(latex_pattern, content):
            protected_blocks.append(ProtectedBlock(
                type="formula",
                content=match.group(),
                start=match.start(),
                end=match.end()
            ))

        return protected_blocks

class GraphExtractor:
    """çŸ¥è¯†å›¾è°±æŠ½å–æœåŠ¡"""

    def __init__(self, llm_service: LLMService):
        self.llm_service = llm_service
        self.entity_types = [
            "PERSON", "ORGANIZATION", "LOCATION", "PRODUCT",
            "CONCEPT", "EVENT", "DATE", "MONEY", "TECHNOLOGY"
        ]

    async def extract_entities_and_relations(self, chunks: List[DocumentChunk]) -> GraphData:
        """ä»æ–‡æ¡£å—ä¸­æŠ½å–å®ä½“å’Œå…³ç³»"""

        all_entities = []
        all_relations = []

        for chunk in chunks:
            # å®ä½“æŠ½å–
            entities = await self._extract_entities(chunk.content)

            # å…³ç³»æŠ½å–
            relations = await self._extract_relations(chunk.content, entities)

            # æ·»åŠ å…ƒæ•°æ®
            for entity in entities:
                entity.source_chunk_id = chunk.id
                entity.confidence = self._calculate_entity_confidence(entity, chunk)

            for relation in relations:
                relation.source_chunk_id = chunk.id
                relation.confidence = self._calculate_relation_confidence(relation, chunk)

            all_entities.extend(entities)
            all_relations.extend(relations)

        # å®ä½“æ¶ˆæ­§å’Œåˆå¹¶
        merged_entities = await self._merge_duplicate_entities(all_entities)

        # å…³ç³»éªŒè¯å’Œè¿‡æ»¤
        validated_relations = await self._validate_relations(all_relations, merged_entities)

        return GraphData(
            entities=merged_entities,
            relations=validated_relations
        )

    async def _extract_entities(self, text: str) -> List[Entity]:
        """ä½¿ç”¨LLMæŠ½å–å®ä½“"""

        prompt = f"""
        ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æŠ½å–å®ä½“ï¼Œå¹¶åˆ†ç±»åˆ°æŒ‡å®šç±»å‹ä¸­ã€‚

        å®ä½“ç±»å‹ï¼š{', '.join(self.entity_types)}

        æ–‡æœ¬ï¼š
        {text}

        è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
        {{
            "entities": [
                {{
                    "name": "å®ä½“åç§°",
                    "type": "å®ä½“ç±»å‹",
                    "description": "å®ä½“æè¿°",
                    "mentions": ["æåŠ1", "æåŠ2"]
                }}
            ]
        }}
        """

        response = await self.llm_service.generate(prompt)
        result = json.loads(response)

        entities = []
        for entity_data in result["entities"]:
            entities.append(Entity(
                name=entity_data["name"],
                type=entity_data["type"],
                description=entity_data.get("description", ""),
                mentions=entity_data.get("mentions", [])
            ))

        return entities

    async def _extract_relations(self, text: str, entities: List[Entity]) -> List[Relation]:
        """æŠ½å–å®ä½“é—´çš„å…³ç³»"""

        entity_names = [entity.name for entity in entities]

        prompt = f"""
        ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æŠ½å–å®ä½“é—´çš„å…³ç³»ã€‚

        å·²è¯†åˆ«çš„å®ä½“ï¼š{', '.join(entity_names)}

        æ–‡æœ¬ï¼š
        {text}

        è¯·ä»¥JSONæ ¼å¼è¿”å›å…³ç³»ï¼š
        {{
            "relations": [
                {{
                    "subject": "ä¸»ä½“å®ä½“",
                    "predicate": "å…³ç³»ç±»å‹",
                    "object": "å®¢ä½“å®ä½“",
                    "description": "å…³ç³»æè¿°"
                }}
            ]
        }}
        """

        response = await self.llm_service.generate(prompt)
        result = json.loads(response)

        relations = []
        for relation_data in result["relations"]:
            relations.append(Relation(
                subject=relation_data["subject"],
                predicate=relation_data["predicate"],
                object=relation_data["object"],
                description=relation_data.get("description", "")
            ))

        return relations
```

## ğŸš€ éƒ¨ç½²æ¶æ„è®¾è®¡

### Dockerå®¹å™¨åŒ–
```dockerfile
# åç«¯æœåŠ¡Dockerfile
FROM python:3.10-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  # åç«¯APIæœåŠ¡
  api:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=mysql://user:password@mysql:3306/ragdb
      - MILVUS_HOST=milvus
      - NEO4J_URI=bolt://neo4j:7687
      - REDIS_URL=redis://redis:6379
    depends_on:
      - mysql
      - milvus
      - neo4j
      - redis
    volumes:
      - ./uploads:/app/uploads
      - ./logs:/app/logs

  # å‰ç«¯æœåŠ¡
  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
    depends_on:
      - api

  # MySQLæ•°æ®åº“
  mysql:
    image: mysql:8.0
    environment:
      - MYSQL_ROOT_PASSWORD=rootpassword
      - MYSQL_DATABASE=ragdb
      - MYSQL_USER=user
      - MYSQL_PASSWORD=password
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql

  # Milvuså‘é‡æ•°æ®åº“
  milvus:
    image: milvusdb/milvus:v2.4.0
    command: ["milvus", "run", "standalone"]
    environment:
      - ETCD_ENDPOINTS=etcd:2379
      - MINIO_ADDRESS=minio:9000
    ports:
      - "19530:19530"
    depends_on:
      - etcd
      - minio
    volumes:
      - milvus_data:/var/lib/milvus

  # Neo4jå›¾æ•°æ®åº“
  neo4j:
    image: neo4j:5.15
    environment:
      - NEO4J_AUTH=neo4j/password
      - NEO4J_PLUGINS=["apoc"]
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4j_data:/data

  # Redisç¼“å­˜
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  # MinIOå¯¹è±¡å­˜å‚¨
  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data

  # Etcd (Milvusä¾èµ–)
  etcd:
    image: quay.io/coreos/etcd:v3.5.5
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    volumes:
      - etcd_data:/etcd

volumes:
  mysql_data:
  milvus_data:
  neo4j_data:
  redis_data:
  minio_data:
  etcd_data:
```

### Kubernetesç”Ÿäº§éƒ¨ç½²
```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: rag-api
  template:
    metadata:
      labels:
        app: rag-api
    spec:
      containers:
      - name: api
        image: rag-system/api:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: rag-api-service
spec:
  selector:
    app: rag-api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```
```
